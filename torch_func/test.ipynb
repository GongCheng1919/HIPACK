{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess time: 0.00011837s\n",
      "direct_conv2d computation time: 1.20312s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import direct_conv2d  # 确保已经导入direct_conv2d模块\n",
    "\n",
    "import time\n",
    "import logging\n",
    "\n",
    "inp = torch.rand((16,256,56,56)).int()\n",
    "x = torch.rand((256,256,3,3)).int()\n",
    "OUT = direct_conv2d.direct_conv2d(inp, x,3,3,1,1,1,0)\n",
    "# 如果包中有描述信息的属性（这不是标准，可能不存在）\n",
    "# print(direct_conv2d.__description__)  # 这行可能不会工作，因为__description__不是一个标准属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import direct_conv2d\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import direct_conv2d  # 确保已经导入direct_conv2d模块\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch.nn.quantized\n",
    "\n",
    "class MeasureExecutionTime:\n",
    "    def __init__(self, measure_name = \"Execution\", type='auto', log_to_file=False, log_file='./execution_time.log'):\n",
    "        self.log_to_file = log_to_file\n",
    "        self.log_file = log_file\n",
    "        self.start_time = None\n",
    "        self.measure_name = measure_name\n",
    "        self.type=type\n",
    "        self.type_convert = {\"auto\":0,\"s\":1,\"ms\":1000,\"us\":1000000,\"ns\":1000000000}\n",
    "        if self.type not in self.type_convert.keys():\n",
    "            raise ValueError(f\"Invalid type {self.type}, only support {self.type_convert.keys()}\")\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - self.start_time\n",
    "        type = self.type\n",
    "        if type == \"auto\":\n",
    "            # 自动选择时间单位\n",
    "            if execution_time > 1:\n",
    "                type = \"s\"\n",
    "            elif execution_time > 0.001:\n",
    "                type = \"ms\"\n",
    "            elif execution_time > 0.000001:\n",
    "                type = \"us\"\n",
    "            else:\n",
    "                type = \"ns\"\n",
    "        display_time = execution_time*self.type_convert[type]\n",
    "        print(f\"{self.measure_name} time: {display_time:.4f} {type}\")\n",
    "        if self.log_to_file:\n",
    "            logging.basicConfig(filename=self.log_file, level=logging.INFO)\n",
    "            logging.info(f\"Execution time: {display_time:.4f} {type}\")\n",
    "\n",
    "\n",
    "def direct_conv2d_func(inp, weight, W_bits, A_bits, MT):\n",
    "    return direct_conv2d.direct_conv2d(inp, weight, W_bits, A_bits, MT)\n",
    "\n",
    "class DirectConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "                 kernel_size, W_bits, A_bits, \n",
    "                 MT=True, padding=0, \n",
    "                 stride=1, dilation=1,\n",
    "                 prepare_func=lambda x: x.int(),\n",
    "                 post_func = lambda x: x,\n",
    "                 measure_time=False):\n",
    "        super(DirectConv2d, self).__init__()\n",
    "        self.prepare_func = prepare_func\n",
    "        self.post_func = post_func\n",
    "        self.measure_time = measure_time\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.W_bits = W_bits\n",
    "        self.A_bits = A_bits\n",
    "        self.MT = MT\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        max_w = 2**(W_bits) - 1\n",
    "        max_a = 2**(A_bits) - 1\n",
    "        regA = 2\n",
    "        TN =2\n",
    "        if (W_bits+A_bits)<3:\n",
    "            TN = 4\n",
    "        self.align_num = regA*self.kernel_size*TN\n",
    "        self.weight = torch.randint(0, max_w, \n",
    "                               (self.out_channels, \n",
    "                                self.in_channels, \n",
    "                                self.kernel_size,\n",
    "                                self.kernel_size)).int()\n",
    "        \n",
    "        # 初始化权重，这里假设权重是正方形的\n",
    "        # 注意：权重的数据类型应该与期望的整型匹配\n",
    "        # self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size).to(torch.int32))\n",
    "        # self.weight = torch.randn(out_channels, in_channels, kernel_size, kernel_size).int()\n",
    "    def set_quant_bits(self, W_bits,A_bits):\n",
    "        self.W_bits = W_bits\n",
    "        self.A_bits = A_bits\n",
    "        max_w = 2**(W_bits) - 1\n",
    "        max_a = 2**(A_bits) - 1\n",
    "        self.weight = torch.randint(0, max_w, \n",
    "                               (self.out_channels, \n",
    "                                self.in_channels, \n",
    "                                self.kernel_size,\n",
    "                                self.kernel_size)).int()\n",
    "        regA = 2\n",
    "        TN =2\n",
    "        if (W_bits+A_bits)<3:\n",
    "            TN = 4\n",
    "        self.align_num = self.kernel_size*TN\n",
    "\n",
    "    def evaluate_perf(self,output_shape,eval_time):\n",
    "        self.flops = np.prod(output_shape)*np.prod(self.weight.shape)*2/self.in_channels\n",
    "        perf = self.flops/eval_time\n",
    "        # auto set 单位\n",
    "        if perf>1e12:\n",
    "            perf = perf/1e12\n",
    "            unit = \"TFLOPS\"\n",
    "        elif perf>1e9:\n",
    "            perf = perf/1e9\n",
    "            unit = \"GFLOPS\"\n",
    "        elif perf>1e6:\n",
    "            perf = perf/1e6\n",
    "            unit = \"MFLOPS\"\n",
    "        elif perf>1e3:\n",
    "            perf = perf/1e3\n",
    "            unit = \"KFLOPS\"\n",
    "        return f\"{perf:.2f}{unit}\"\n",
    "\n",
    "    def forward(self,inp):\n",
    "        if self.measure_time:\n",
    "            return self.measure_forward(inp)\n",
    "        else:\n",
    "            return self.direct_forward(inp)\n",
    "    \n",
    "    def direct_forward(self,inp):\n",
    "        # 检查inp和weight的数据类型是否为整型\n",
    "        if not inp.dtype == torch.int32:\n",
    "            # raise TypeError(\"inp and weight must be of int32 type\")\n",
    "            inp = self.prepare_func(inp)\n",
    "        # 检查inp和weight的形状是否匹配，不匹配则出错：\n",
    "        if not inp.size(1) == self.in_channels:\n",
    "            raise ValueError(f\"input shape {inp.shape} does not match weight shape {self.weight.shape}\")\n",
    "        # 此处还需要对inp做padding用于计算\n",
    "        W = inp.size(3)\n",
    "        W = int((W//self.align_num)*self.align_num+((W%self.align_num)>0)*self.align_num) \n",
    "        # employ the new W to padding inp (with zero)\n",
    "        if W!=inp.size(3):\n",
    "            inp = torch.nn.functional.pad(inp, (0, W-inp.size(3), 0, 0), mode='constant', value=0)\n",
    "        # 调用direct_conv2d.direct_conv2d函数\n",
    "        # print(inp.shape,self.weight.shape)\n",
    "        # start = time.perf_counter()\n",
    "        # direct_conv2d_func(inp, self.weight, self.W_bits, self.A_bits, self.MT)\n",
    "        output =  direct_conv2d.direct_conv2d(inp, self.weight, self.W_bits, self.A_bits, self.MT,1,0,0)\n",
    "        # eclapsed = time.perf_counter()-start\n",
    "        # print(output.shape,f\"time eclapsed: {eclapsed}s\")\n",
    "        # self.full_perf = self.evaluate_perf(output.shape,eclapsed)\n",
    "        # 如果输出只要中间的一块，需要对output进行裁剪，其从0位置开始，裁剪出和inp一样大的区域\n",
    "        # output = self.post_func(output[:, :, 1:1+inp.size(2), 1:1+inp.size(2)])\n",
    "        # print(output.shape)\n",
    "        # self.cutted_perf = self.evaluate_perf(output.shape,eclapsed)\n",
    "        return output\n",
    "    \n",
    "    def measure_forward(self, inp):\n",
    "        with MeasureExecutionTime(measure_name=\"Check inp type and shape\"):\n",
    "            # 检查inp和weight的数据类型是否为整型\n",
    "            if not inp.dtype == torch.int32:\n",
    "                # raise TypeError(\"inp and weight must be of int32 type\")\n",
    "                inp = self.prepare_func(inp)\n",
    "            # 检查inp和weight的形状是否匹配，不匹配则出错：\n",
    "            if not inp.size(1) == self.in_channels:\n",
    "                raise ValueError(f\"input shape {inp.shape} does not match weight shape {self.weight.shape}\")\n",
    "        with MeasureExecutionTime(measure_name=\"Padding inp\"):\n",
    "            # 此处还需要对inp做padding用于计算\n",
    "            W = inp.size(3)\n",
    "            W = int((W//self.align_num)*self.align_num+((W%self.align_num)>0)*self.align_num) \n",
    "            # employ the new W to padding inp (with zero)\n",
    "            inp = torch.nn.functional.pad(inp, (0, W-inp.size(3), 0, 0), mode='constant', value=0)\n",
    "        with MeasureExecutionTime(measure_name=\"Direct Conv2d Excution\"):\n",
    "            # 调用direct_conv2d.direct_conv2d函数\n",
    "            output = direct_conv2d_func(inp, self.weight, self.W_bits, self.A_bits, self.MT)\n",
    "        with MeasureExecutionTime(measure_name=\"Post process\"):\n",
    "            # 如果输出只要中间的一块，需要对output进行裁剪，其从0位置开始，裁剪出和inp一样大的区域\n",
    "            output = self.post_func(output[:, :, 1:1+inp.size(2), 1:1+inp.size(2)])\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def from_conv2d(conv2d, W_bits, A_bits, MT=True, measure_time=False):\n",
    "        # 从一个标准的Conv2d层构造一个DirectConv2d层\n",
    "        return DirectConv2d(conv2d.in_channels, conv2d.out_channels, \n",
    "                            conv2d.kernel_size[0], W_bits, A_bits, MT, \n",
    "                            padding=conv2d.padding[0], stride=conv2d.stride[0], \n",
    "                            dilation=conv2d.dilation[0], measure_time=measure_time)\n",
    "    \n",
    "class PadConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, \n",
    "                kernel_size, \n",
    "                W_bits=3,A_bits=3,\n",
    "                stride=1, \n",
    "                padding=0, dilation=1, \n",
    "                groups=1, bias=False):\n",
    "        super(PadConv2d, self).__init__()\n",
    "        self.align_num = 6\n",
    "        if W_bits+A_bits<3:\n",
    "            self.align_num = 12\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, \n",
    "                kernel_size, stride=stride, \n",
    "                padding=2, dilation=dilation, \n",
    "                groups=groups, bias=bias)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # with MeasureExecutionTime(measure_name=\"Padding inp\"):\n",
    "        # 此处还需要对inp做padding用于计算\n",
    "        W = inp.size(3)\n",
    "        W = int((W//self.align_num)*self.align_num+((W%self.align_num)>0)*self.align_num) \n",
    "        # employ the new W to padding inp (with zero)\n",
    "        if W!=inp.size(3):\n",
    "            inp = torch.nn.functional.pad(inp, (0, W-inp.size(3), 0, 0), mode='constant', value=0)\n",
    "        output = self.conv(inp)\n",
    "        return output[:, :, 1:1+inp.size(2), 1:1+inp.size(2)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_conv2d(conv2d, W_bits=3, A_bits=3):\n",
    "        # 从一个标准的Conv2d层构造一个PadConv2d层\n",
    "        return PadConv2d(conv2d.in_channels, conv2d.out_channels, \n",
    "                        conv2d.kernel_size[0], W_bits, A_bits, \n",
    "                        padding=2, stride=conv2d.stride[0], \n",
    "                        dilation=conv2d.dilation[0])\n",
    "\n",
    "class Qint8Conv2D(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, out_channels, \n",
    "                 kernel_size, stride=1, \n",
    "                 padding=0, dilation=1, \n",
    "                 groups=1, bias=None,return_float=False,engine=\"qnnpack\"):\n",
    "        super(Qint8Conv2D, self).__init__()\n",
    "        torch.backends.quantized.engine = engine\n",
    "        # self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "        self.conv = torch.nn.quantized.functional.conv2d\n",
    "        # self.conv.weight.data = self.conv.weight.data.int()\n",
    "        self.quant = lambda x: torch.quantize_per_tensor(x, 1, 0, torch.quint8)\n",
    "        weight = torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        self.weight = torch.quantize_per_tensor(weight, 1, 0, torch.qint8)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.bias = bias\n",
    "        self.return_float = return_float\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # 检查，如果不是quint8的类型，则转换一下\n",
    "        if not inp.dtype == torch.quint8:\n",
    "            inp = self.quant(inp)\n",
    "        output = self.conv(inp,self.weight,self.bias,\n",
    "                           padding=self.padding, stride=self.stride,\n",
    "                           groups=self.groups,dilation=self.dilation,\n",
    "                           scale=1.,zero_point=0)\n",
    "        if self.return_float:\n",
    "            output = torch.dequantize(output)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_float_conv2d(conv2d,copy_weight=True):\n",
    "        # 从一个标准的Conv2d层构造一个Qint8Conv2D层\n",
    "        qconv = Qint8Conv2D(conv2d.in_channels, conv2d.out_channels, \n",
    "                        conv2d.kernel_size[0], conv2d.stride[0], conv2d.padding[0], \n",
    "                        conv2d.dilation[0], conv2d.groups, conv2d.bias)\n",
    "        if copy_weight:\n",
    "            qconv.weight = torch.quantize_per_tensor(conv2d.weight.data, 1, 0, torch.qint8)\n",
    "        return qconv\n",
    "\n",
    "class PadQint8Conv2D(nn.Module):\n",
    "    def __init__(self, \n",
    "                in_channels, out_channels, \n",
    "                kernel_size, \n",
    "                W_bits=3,A_bits=3,\n",
    "                stride=1, \n",
    "                padding=0, dilation=1, \n",
    "                groups=1, bias=None,return_float=False):\n",
    "        super(PadQint8Conv2D, self).__init__()\n",
    "        self.conv = Qint8Conv2D(in_channels, out_channels, kernel_size, \n",
    "                                stride, padding=2, dilation=dilation, groups=groups, \n",
    "                                bias=bias,return_float=return_float)\n",
    "        self.align_num = 6\n",
    "        if (W_bits+A_bits)<3:\n",
    "            self.align_num = 12\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # 此处还需要对inp做padding用于计算\n",
    "        W = inp.size(3)\n",
    "        W = int((W//self.align_num)*self.align_num+((W%self.align_num)>0)*self.align_num) \n",
    "        # employ the new W to padding inp (with zero)\n",
    "        if W!=inp.size(3):\n",
    "            if inp.dtype == torch.quint8:\n",
    "                inp = torch.dequantize(inp)\n",
    "            inp = torch.nn.functional.pad(inp, (0, W-inp.size(3), 0, 0), mode='constant', value=0)\n",
    "        output = self.conv(inp)\n",
    "        return output[:, :, 1:1+inp.size(2), 1:1+inp.size(2)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_float_conv2d(conv2d,W_bits=3,A_bits=3,copy_weight=True):\n",
    "        # 从一个标准的Conv2d层构造一个PadQint8Conv2D层\n",
    "        qconv = PadQint8Conv2D(conv2d.in_channels, conv2d.out_channels, \n",
    "                        conv2d.kernel_size[0], W_bits, A_bits,\n",
    "                        conv2d.stride[0], conv2d.padding[0], \n",
    "                        conv2d.dilation[0], conv2d.groups, conv2d.bias)\n",
    "        if copy_weight:\n",
    "            qconv.conv.weight = torch.quantize_per_tensor(conv2d.weight.data, 1, 0, torch.qint8)\n",
    "        return qconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float Conv2d time: 9.4674 ms\n",
      "Qint8 Conv2d time: 7.5815 ms\n",
      "Direct Conv2d time: 10.1037 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import direct_conv2d  # 确保已经导入direct_conv2d模块\n",
    "import time\n",
    "# Test Direct Conv2D perf\n",
    "N, Ci, H, W, Co, W_bits,A_bits,MT =1,8,12,12,8,3,3,1\n",
    "inp = torch.randint(0, 2**A_bits -1, (N, Ci, H, W)).float()\n",
    "weight = torch.randint(0, 2**W_bits -1, (Co, Ci, 3, 3)).float()\n",
    "fconv = nn.Conv2d(Ci, Co, 3, padding=2)\n",
    "qconv = Qint8Conv2D(Ci, Co, 3,padding=2)\n",
    "dconv = DirectConv2d(Ci, Co, 3, W_bits, A_bits, MT, measure_time=False)\n",
    "fconv.weight.data.copy_(weight)\n",
    "dconv.weight.data.copy_(weight.int())\n",
    "# qconv.weight.data.copy_(weight.int())\n",
    "\n",
    "# padfconv = PadConv2d(Ci, Co, 3, W_bits, A_bits)\n",
    "# padqconv = PadQint8Conv2D(Ci, Co, 3, W_bits, A_bits)\n",
    "\n",
    "with MeasureExecutionTime(measure_name=\"Float Conv2d\"):\n",
    "    output_f = fconv(inp)\n",
    "\n",
    "with MeasureExecutionTime(measure_name=\"Qint8 Conv2d\"):\n",
    "    output_q8 = qconv(inp)\n",
    "\n",
    "\n",
    "with MeasureExecutionTime(measure_name=\"Direct Conv2d\"):\n",
    "    output_h3 = dconv(inp)\n",
    "\n",
    "# with MeasureExecutionTime(measure_name=\"Pad Float Conv2d\"):\n",
    "#     output_pf = padfconv(inp)\n",
    "    \n",
    "# with MeasureExecutionTime(measure_name=\"Pad Qint8 Conv2d\"):\n",
    "#     output_pq8 = padqconv(inp)\n",
    "\n",
    "# output = conv(inp)\n",
    "# print(f\"Direct Conv2d full perf: {conv.full_perf}, cutted perf: {conv.cutted_perf}\")\n",
    "\n",
    "# weight = torch.randint(0, 2**W_bits -1, (Co, Ci, 3, 3)).int()\n",
    "# print(inp.shape,weight.shape)\n",
    "# start = time.perf_counter()\n",
    "# # output = direct_conv2d_func(inp, weight, W_bits, A_bits, 1, 0, 1, 1)\n",
    "# output = direct_conv2d.direct_conv2d(inp, weight, W_bits, A_bits, 1)\n",
    "# eclapsed = time.perf_counter()-start\n",
    "# print(output.shape,f\"time eclapsed: {eclapsed}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[4.0000e+01, 1.1000e+02, 2.2200e+02,  ..., 1.8700e+02,\n",
       "           1.6900e+02, 8.5000e+01],\n",
       "          [1.3900e+02, 2.6300e+02, 4.4400e+02,  ..., 4.5500e+02,\n",
       "           3.6900e+02, 2.0700e+02],\n",
       "          [1.8900e+02, 3.6600e+02, 6.3000e+02,  ..., 6.3500e+02,\n",
       "           5.7200e+02, 2.5800e+02],\n",
       "          ...,\n",
       "          [1.7700e+02, 4.1200e+02, 6.5900e+02,  ..., 6.0000e+02,\n",
       "           4.7200e+02, 2.5500e+02],\n",
       "          [1.2900e+02, 3.2100e+02, 4.5400e+02,  ..., 4.2100e+02,\n",
       "           3.0400e+02, 1.3000e+02],\n",
       "          [2.9000e+01, 1.4300e+02, 1.9600e+02,  ..., 2.1000e+02,\n",
       "           1.5500e+02, 9.4000e+01]],\n",
       "\n",
       "         [[5.2000e+01, 1.4100e+02, 2.2800e+02,  ..., 2.4300e+02,\n",
       "           1.7500e+02, 6.6000e+01],\n",
       "          [1.2200e+02, 2.5100e+02, 4.4600e+02,  ..., 4.4500e+02,\n",
       "           3.6100e+02, 1.4300e+02],\n",
       "          [2.2100e+02, 4.4300e+02, 6.6300e+02,  ..., 7.4700e+02,\n",
       "           5.7200e+02, 2.5200e+02],\n",
       "          ...,\n",
       "          [1.8300e+02, 4.5500e+02, 6.7500e+02,  ..., 6.7300e+02,\n",
       "           5.1700e+02, 2.0500e+02],\n",
       "          [1.3300e+02, 3.1800e+02, 4.4900e+02,  ..., 5.0600e+02,\n",
       "           3.1200e+02, 1.3500e+02],\n",
       "          [7.3000e+01, 1.7100e+02, 2.3800e+02,  ..., 2.2700e+02,\n",
       "           1.7500e+02, 7.2000e+01]],\n",
       "\n",
       "         [[7.4000e+01, 1.4300e+02, 1.8000e+02,  ..., 2.1700e+02,\n",
       "           1.4300e+02, 6.0000e+01],\n",
       "          [1.2700e+02, 2.7000e+02, 4.4500e+02,  ..., 4.5300e+02,\n",
       "           3.3800e+02, 1.9900e+02],\n",
       "          [2.4100e+02, 4.3100e+02, 6.6200e+02,  ..., 7.4300e+02,\n",
       "           5.6500e+02, 2.7900e+02],\n",
       "          ...,\n",
       "          [2.3800e+02, 4.2000e+02, 6.9600e+02,  ..., 7.1600e+02,\n",
       "           4.7400e+02, 2.7100e+02],\n",
       "          [1.4600e+02, 2.7300e+02, 5.2700e+02,  ..., 4.9800e+02,\n",
       "           3.4700e+02, 1.7800e+02],\n",
       "          [8.0000e+01, 1.5900e+02, 2.2200e+02,  ..., 2.7500e+02,\n",
       "           1.3200e+02, 1.0400e+02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[3.8000e+01, 1.1800e+02, 2.2100e+02,  ..., 1.7900e+02,\n",
       "           1.6700e+02, 5.1000e+01],\n",
       "          [1.2600e+02, 3.2900e+02, 5.3200e+02,  ..., 5.5900e+02,\n",
       "           4.5000e+02, 2.0600e+02],\n",
       "          [2.2400e+02, 4.7100e+02, 7.0900e+02,  ..., 8.1700e+02,\n",
       "           6.2200e+02, 3.1400e+02],\n",
       "          ...,\n",
       "          [2.3600e+02, 4.9700e+02, 7.7800e+02,  ..., 8.0300e+02,\n",
       "           5.6000e+02, 2.6300e+02],\n",
       "          [1.7900e+02, 3.2800e+02, 5.8200e+02,  ..., 5.4800e+02,\n",
       "           3.8600e+02, 1.8300e+02],\n",
       "          [7.8000e+01, 1.5400e+02, 1.9600e+02,  ..., 2.3400e+02,\n",
       "           1.4000e+02, 1.0200e+02]],\n",
       "\n",
       "         [[6.0000e+01, 1.5900e+02, 2.3400e+02,  ..., 2.4600e+02,\n",
       "           1.7600e+02, 7.6000e+01],\n",
       "          [1.2100e+02, 3.1400e+02, 4.9700e+02,  ..., 5.2200e+02,\n",
       "           4.1700e+02, 1.4800e+02],\n",
       "          [1.9700e+02, 4.5200e+02, 6.4500e+02,  ..., 7.5800e+02,\n",
       "           5.0600e+02, 2.4500e+02],\n",
       "          ...,\n",
       "          [2.0900e+02, 4.7200e+02, 7.0100e+02,  ..., 6.5400e+02,\n",
       "           4.9400e+02, 2.4600e+02],\n",
       "          [1.5700e+02, 2.8500e+02, 4.3300e+02,  ..., 4.3200e+02,\n",
       "           2.5300e+02, 1.7300e+02],\n",
       "          [6.8000e+01, 1.4600e+02, 1.6900e+02,  ..., 1.5900e+02,\n",
       "           1.2400e+02, 4.4000e+01]],\n",
       "\n",
       "         [[9.0000e+01, 1.6300e+02, 2.2100e+02,  ..., 2.6400e+02,\n",
       "           1.9700e+02, 1.0500e+02],\n",
       "          [2.1100e+02, 3.9600e+02, 5.2700e+02,  ..., 5.2100e+02,\n",
       "           3.9500e+02, 1.7300e+02],\n",
       "          [3.0600e+02, 5.5700e+02, 3.9627e+37,  ..., 8.6800e+02,\n",
       "           5.4900e+02, 2.4000e+02],\n",
       "          ...,\n",
       "          [2.8000e+02, 5.7200e+02, 8.0300e+02,  ..., 7.9700e+02,\n",
       "           4.5600e+02, 2.1800e+02],\n",
       "          [1.7700e+02, 3.6600e+02, 5.0600e+02,  ..., 5.1000e+02,\n",
       "           3.4100e+02, 1.2200e+02],\n",
       "          [6.3000e+01, 1.6000e+02, 1.8500e+02,  ..., 2.0000e+02,\n",
       "           1.6700e+02, 8.4000e+01]]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 39.9623, 109.9623, 221.9623,  ..., 186.9623, 168.9623,  84.9623],\n",
       "          [138.9623, 262.9623, 443.9623,  ..., 454.9623, 368.9623, 206.9623],\n",
       "          [188.9623, 365.9623, 629.9623,  ..., 634.9623, 571.9623, 257.9623],\n",
       "          ...,\n",
       "          [176.9623, 411.9623, 658.9623,  ..., 599.9623, 471.9623, 254.9623],\n",
       "          [128.9623, 320.9623, 453.9623,  ..., 420.9623, 303.9623, 129.9623],\n",
       "          [ 28.9623, 142.9623, 195.9623,  ..., 209.9623, 154.9623,  93.9623]],\n",
       "\n",
       "         [[ 52.0126, 141.0126, 228.0126,  ..., 243.0126, 175.0126,  66.0126],\n",
       "          [122.0126, 251.0126, 446.0125,  ..., 445.0125, 361.0125, 143.0126],\n",
       "          [221.0126, 443.0125, 663.0126,  ..., 747.0126, 572.0126, 252.0126],\n",
       "          ...,\n",
       "          [183.0126, 455.0125, 675.0126,  ..., 673.0126, 517.0126, 205.0126],\n",
       "          [133.0126, 318.0125, 449.0125,  ..., 506.0125, 312.0125, 135.0126],\n",
       "          [ 73.0126, 171.0126, 238.0126,  ..., 227.0126, 175.0126,  72.0126]],\n",
       "\n",
       "         [[ 73.9063, 142.9063, 179.9063,  ..., 216.9063, 142.9063,  59.9063],\n",
       "          [126.9063, 269.9063, 444.9063,  ..., 452.9063, 337.9063, 198.9063],\n",
       "          [240.9063, 430.9063, 661.9063,  ..., 742.9063, 564.9063, 278.9063],\n",
       "          ...,\n",
       "          [237.9063, 419.9063, 695.9063,  ..., 715.9063, 473.9063, 270.9063],\n",
       "          [145.9063, 272.9063, 526.9063,  ..., 497.9063, 346.9063, 177.9063],\n",
       "          [ 79.9063, 158.9063, 221.9063,  ..., 274.9063, 131.9063, 103.9063]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 37.9521, 117.9521, 220.9521,  ..., 178.9521, 166.9521,  50.9521],\n",
       "          [125.9521, 328.9521, 531.9521,  ..., 558.9521, 449.9521, 205.9521],\n",
       "          [223.9521, 470.9521, 708.9521,  ..., 816.9521, 621.9521, 313.9521],\n",
       "          ...,\n",
       "          [235.9521, 496.9521, 777.9521,  ..., 802.9521, 559.9521, 262.9521],\n",
       "          [178.9521, 327.9521, 581.9521,  ..., 547.9521, 385.9521, 182.9521],\n",
       "          [ 77.9521, 153.9521, 195.9521,  ..., 233.9521, 139.9521, 101.9521]],\n",
       "\n",
       "         [[ 59.9086, 158.9086, 233.9086,  ..., 245.9086, 175.9086,  75.9086],\n",
       "          [120.9086, 313.9086, 496.9086,  ..., 521.9086, 416.9086, 147.9086],\n",
       "          [196.9086, 451.9086, 644.9086,  ..., 757.9086, 505.9086, 244.9086],\n",
       "          ...,\n",
       "          [208.9086, 471.9086, 700.9086,  ..., 653.9086, 493.9086, 245.9086],\n",
       "          [156.9086, 284.9086, 432.9086,  ..., 431.9086, 252.9086, 172.9086],\n",
       "          [ 67.9086, 145.9086, 168.9086,  ..., 158.9086, 123.9086,  43.9086]],\n",
       "\n",
       "         [[ 89.9809, 162.9809, 220.9809,  ..., 263.9809, 196.9809, 104.9809],\n",
       "          [210.9809, 395.9809, 526.9808,  ..., 520.9808, 394.9809, 172.9809],\n",
       "          [305.9809, 556.9808, 700.9808,  ..., 867.9808, 548.9808, 239.9809],\n",
       "          ...,\n",
       "          [279.9809, 571.9808, 802.9808,  ..., 796.9808, 455.9809, 217.9809],\n",
       "          [176.9809, 365.9809, 505.9809,  ..., 509.9809, 340.9809, 121.9809],\n",
       "          [ 62.9809, 159.9809, 184.9809,  ..., 199.9809, 166.9809,  83.9809]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 6, 12]) torch.Size([512, 512, 3, 3])\n",
      "direct_conv2d computation time: 0.0386138s\n",
      "torch.Size([16, 512, 8, 14]) time eclapsed: 0.16410746285691857s GFPLOPS: 51.52548651228823\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import torch.nn as nn\n",
    "# import direct_conv2d  # 确保已经导入direct_conv2d模块\n",
    "# import time\n",
    "# import numpy as np\n",
    "# # Test Direct Conv2D perf\n",
    "# N, Ci, H, W, Co, W_bits,A_bits,MT =16,512,6,12,512,5,5,1\n",
    "# inp = torch.randint(0, 2**A_bits -1, (N, Ci, H, W)).int()\n",
    "# weight = torch.randint(0, 2**W_bits -1, (Co, Ci, 3, 3)).int()\n",
    "print(inp.shape,weight.shape)\n",
    "start = time.perf_counter()\n",
    "# output = direct_conv2d_func(inp, weight, W_bits, A_bits, 1, 0, 1, 1)\n",
    "output = direct_conv2d.direct_conv2d(inp, weight, W_bits, A_bits, 1)\n",
    "# output = F.conv2d(inp.float(), weight.float(),padding=2)\n",
    "eclapsed = time.perf_counter()-start\n",
    "print(output.shape,f\"time eclapsed: {eclapsed}s GFPLOPS: {np.prod(output.shape)*np.prod(weight.shape)*2/weight.shape[1]/eclapsed/1e9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_model_quant_bits\n",
    "def set_model_quant_bits(model, W_bits, A_bits):\n",
    "\tfor name, module in model.named_children():\n",
    "\t\tif isinstance(module, DirectConv2d):\n",
    "\t\t\tmodule.set_quant_bits(W_bits, A_bits)\n",
    "\t\telse:\n",
    "\t\t\tset_model_quant_bits(module, W_bits, A_bits)\n",
    "\treturn model\n",
    "# convert the float conv module to DirectConv2d\n",
    "def convert_to_direct_conv2d(model, W_bits, A_bits, \n",
    "\t\t\t\t\t\t\t module_type=None,\n",
    "\t\t\t\t\t\t\t module_type_name = None, \n",
    "\t\t\t\t\t\t\t prepare_func=lambda x: x.int(),\n",
    "\t\t\t\t\t\t\t post_func=lambda x: x,\n",
    "\t\t\t\t\t\t\t copy_weight = False,\n",
    "\t\t\t\t\t\t\t verbose = True):\n",
    "\t\n",
    "\tfor name, module in model.named_children():\n",
    "\t\t# 分别判断module_type 或者module_type_name来确定是否是该类型\n",
    "\t\tis_this_type = False\n",
    "\t\tif module_type is not None and isinstance(module, module_type):\n",
    "\t\t\tis_this_type = True\n",
    "\t\telif module_type_name is not None and type(module).__name__ == module_type_name:\n",
    "\t\t\tis_this_type = True\n",
    "\t\t# print(f\"module name: {name}, module type: {type(module).__name__}, is_this_type: {is_this_type}\")\n",
    "\t\tif is_this_type and \\\n",
    "\t\t\tmodule.kernel_size[0]==3 and module.padding[0]==1 \\\n",
    "\t\t\t\tand module.stride[0]==1 and module.dilation[0]==1: # 必须要kernel size是3，padding=1, group = 1, 的才能用\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint(f\"convert {name} to DirectConv2d\")\n",
    "\t\t\tdirect_conv2d = DirectConv2d(module.in_channels, \n",
    "\t\t\t\t\t\t\t\tmodule.out_channels, \n",
    "\t\t\t\t\t\t\t\tmodule.kernel_size[0], \n",
    "\t\t\t\t\t\t\t\tW_bits, A_bits, True, \n",
    "\t\t\t\t\t\t\t\tmodule.padding[0], module.stride[0], \n",
    "\t\t\t\t\t\t\t\tmodule.dilation[0], prepare_func,post_func)\n",
    "\t\t\tdirect_conv2d.set_quant_bits(W_bits,A_bits)\n",
    "\t\t\tif copy_weight:\n",
    "\t\t\t\tdirect_conv2d.weight = module.weight\n",
    "\t\t\tsetattr(model, name, direct_conv2d)\n",
    "\t\t\t# setattr(model, name, f\"{name}_direct_conv2d\")\n",
    "\t\telse:\n",
    "\t\t\tconvert_to_direct_conv2d(module, W_bits, A_bits, \n",
    "\t\t\t\t\t\t\tmodule_type, module_type_name,\n",
    "\t\t\t\t\t\t\tprepare_func,post_func,copy_weight,verbose)\n",
    "\treturn model\n",
    "\n",
    "def convert_to_pad_conv2d(model, W_bits, A_bits, \n",
    "\t\t\t\t\t\t\t module_type=None,\n",
    "\t\t\t\t\t\t\t module_type_name = None, \n",
    "\t\t\t\t\t\t\t copy_weight = False,\n",
    "\t\t\t\t\t\t\t verbose = True):\n",
    "\t\n",
    "\tfor name, module in model.named_children():\n",
    "\t\t# 分别判断module_type 或者module_type_name来确定是否是该类型\n",
    "\t\tis_this_type = False\n",
    "\t\tif module_type is not None and isinstance(module, module_type):\n",
    "\t\t\tis_this_type = True\n",
    "\t\telif module_type_name is not None and type(module).__name__ == module_type_name:\n",
    "\t\t\tis_this_type = True\n",
    "\t\t# print(f\"module name: {name}, module type: {type(module).__name__}, is_this_type: {is_this_type}\")\n",
    "\t\tif is_this_type and \\\n",
    "\t\t\tmodule.kernel_size[0]==3 and module.padding[0]==1 \\\n",
    "\t\t\t\tand module.stride[0]==1 and module.dilation[0]==1: # 必须要kernel size是3，padding=1, group = 1, 的才能用\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint(f\"convert {name} to PadConv2d\")\n",
    "\t\t\tpad_conv2d = PadConv2d(module.in_channels, \n",
    "\t\t\t\t\t\t\t\tmodule.out_channels, \n",
    "\t\t\t\t\t\t\t\tmodule.kernel_size[0], \n",
    "\t\t\t\t\t\t\t\tW_bits = W_bits, A_bits = A_bits, \n",
    "\t\t\t\t\t\t\t\tstride= module.stride[0], \n",
    "\t\t\t\t\t\t\t\tdilation=module.dilation[0])\n",
    "\t\t\t# direct_conv2d.set_quant_bits(W_bits,A_bits)\n",
    "\t\t\tif copy_weight:\n",
    "\t\t\t\tpad_conv2d.weight.data.cpoy_(module.weight.data)\n",
    "\t\t\tsetattr(model, name, pad_conv2d)\n",
    "\t\t\t# setattr(model, name, f\"{name}_direct_conv2d\")\n",
    "\t\telse:\n",
    "\t\t\tconvert_to_pad_conv2d(module, W_bits, A_bits, \n",
    "\t\t\t\t\t\t\tmodule_type, module_type_name,\n",
    "\t\t\t\t\t\t\tcopy_weight,verbose)\n",
    "\treturn model\n",
    "\n",
    "class FakeFuseModuleBNReLU(nn.Module):\n",
    "\tdef __init__(self, module):\n",
    "\t\tsuper(FakeFuseModuleBNReLU, self).__init__()\n",
    "\t\tself.module = module\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.module(x)\n",
    "\n",
    "class FakeFuseModuleBN(nn.Module):\n",
    "\tdef __init__(self, module):\n",
    "\t\tsuper(FakeFuseModuleBN, self).__init__()\n",
    "\t\tself.module = module\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.module(x)\n",
    "\n",
    "class FakeFuseModuleReLU(nn.Module):\n",
    "\tdef __init__(self, module):\n",
    "\t\tsuper(FakeFuseModuleReLU, self).__init__()\n",
    "\t\tself.module = module\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.module(x)\n",
    "\n",
    "def fake_fuse_module_bn_relu(model,inplace = False):\n",
    "\tif not inplace:\n",
    "\t\tmodel = copy.deepcopy(model)\n",
    "\tpre_conv2d=False\n",
    "\tpre_linear=False\n",
    "\tpointer_module = None\n",
    "\tpointer_name = \"\"\n",
    "\tnext = 0\n",
    "\tfor name, module in model.named_children():\n",
    "\t\tif isinstance(module, (nn.Linear, nn.Conv2d,DirectConv2d,PadConv2d)):\n",
    "\t\t\tif pre_conv2d or pre_linear:\n",
    "\t\t\t\tif next==1 and pointer_module is not None:\n",
    "\t\t\t\t\tsetattr(model,pointer_name,FakeFuseModuleBN(pointer_module))\n",
    "\n",
    "\t\t\tif isinstance(module, nn.Linear):\n",
    "\t\t\t\tpre_linear = True\n",
    "\t\t\telse:\n",
    "\t\t\t\tpre_conv2d = True\n",
    "\t\t\tpointer_module = module\n",
    "\t\t\tpointer_name = name\n",
    "\t\t\tnext = 0\n",
    "\t\t\t\n",
    "\t\telif isinstance(module, (nn.BatchNorm2d,)):\n",
    "\t\t\tif pre_conv2d or pre_linear:\n",
    "\t\t\t\tsetattr(model, name, nn.Identity())\n",
    "\t\t\t\tnext+=1\n",
    "\t\telif isinstance(module, (nn.ReLU, )):\n",
    "\t\t\tif pre_conv2d or pre_linear:\n",
    "\t\t\t\tsetattr(model, name, nn.Identity())\n",
    "\t\t\t\tnext+=1\n",
    "\t\t\t\tif next==1 and pointer_module is not None:\n",
    "\t\t\t\t\tsetattr(model,pointer_name,FakeFuseModuleReLU(pointer_module))\n",
    "\t\t\t\telif next==2 and pointer_module is not None:\n",
    "\t\t\t\t\tsetattr(model,pointer_name,FakeFuseModuleBNReLU(pointer_module))\n",
    "\t\t\t\tpre_linear = False\n",
    "\t\t\t\tpre_conv2d = False\n",
    "\t\t\t\tpointer_module = None\n",
    "\t\t\t\tpointer_name = \"\"\n",
    "\t\t\t\tnext = 0\n",
    "\t\t\t\t\n",
    "\t\telse:\n",
    "\t\t\tif pre_conv2d or pre_linear:\n",
    "\t\t\t\tif next==1 and pointer_module is not None:\n",
    "\t\t\t\t\tsetattr(model,pointer_name,FakeFuseModuleBN(pointer_module))\n",
    "\t\t\tpre_linear = False\n",
    "\t\t\tpre_conv2d = False\n",
    "\t\t\tpointer_module = None\n",
    "\t\t\tpointer_name = \"\"\n",
    "\t\t\tnext = 0\n",
    "\t\t\tfake_fuse_module_bn_relu(module,True)\n",
    "\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): FakeFuseModuleBNReLU(\n",
      "    (module): PadConv2d(\n",
      "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (1): Identity()\n",
      "  (2): Identity()\n",
      "  (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (4): FakeFuseModuleReLU(\n",
      "    (module): PadConv2d(\n",
      "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (5): Identity()\n",
      "  (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (8): FakeFuseModuleBN(\n",
      "    (module): PadConv2d(\n",
      "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (9): Identity()\n",
      "  (10): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (11): FakeFuseModuleBNReLU(\n",
      "    (module): Linear(in_features=24, out_features=24, bias=True)\n",
      "  )\n",
      "  (12): Identity()\n",
      "  (13): Identity()\n",
      "  (14): FakeFuseModuleReLU(\n",
      "    (module): Linear(in_features=24, out_features=24, bias=True)\n",
      "  )\n",
      "  (15): Identity()\n",
      "  (16): FakeFuseModuleBN(\n",
      "    (module): Linear(in_features=24, out_features=24, bias=True)\n",
      "  )\n",
      "  (17): Identity()\n",
      "  (18): Linear(in_features=24, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# test_module = nn.Sequential(PadConv2d(3, 64, 3, padding=1),nn.BatchNorm2d(64), nn.ReLU(),\n",
    "# \t\t\t\t\t\t\tnn.AvgPool2d(2,2),\n",
    "# \t\t\t\t\t\t\tPadConv2d(3, 64, 3, padding=1),nn.ReLU(),nn.BatchNorm2d(64), \n",
    "# \t\t\t\t\t\t\tnn.AvgPool2d(2,2),\n",
    "# \t\t\t\t\t\t\tPadConv2d(3, 64, 3, padding=1),nn.BatchNorm2d(64), \n",
    "# \t\t\t\t\t\t\tnn.AvgPool2d(2,2),\n",
    "# \t\t\t\t\t\t\tnn.Linear(24,24),nn.BatchNorm2d(24),nn.ReLU(),\n",
    "# \t\t\t\t\t\t\tnn.Linear(24,24),nn.ReLU(), \n",
    "# \t\t\t\t\t\t\tnn.Linear(24,24),nn.BatchNorm2d(24),\n",
    "# \t\t\t\t\t\t\tnn.Linear(24,10))\n",
    "# fake_fused_module = fake_fuse_module_bn_relu(test_module)\n",
    "# print(fake_fused_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torchvision\n",
    "# import time\n",
    "# from torchvision.models import vgg\n",
    "# from tqdm import tqdm\n",
    "# import copy\n",
    "# from torch.quantization import QConfig\n",
    "# import torch.nn.quantized as nnq\n",
    "# from torch.ao.quantization import (\n",
    "#   get_default_qconfig_mapping,\n",
    "#   get_default_qat_qconfig_mapping,\n",
    "#   QConfigMapping,\n",
    "# )\n",
    "# import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "\n",
    "\n",
    "# # 定义一些工具函数：\n",
    "# def eager_quantize_vgg(model,shape):\n",
    "# \tengine_name = 'qnnpack' # 'fbgemm' # 'qnnpack'\n",
    "# \t# Set the quantization engine to QNNPACK\n",
    "# \ttorch.backends.quantized.engine = engine_name\n",
    "# \t# qconfig = torch.quantization.get_default_qconfig(engine_name)\n",
    "# \tqconfig = QConfig(\n",
    "# \t\tweight=torch.quantization.default_observer.with_args(dtype=torch.qint8),  # Use quint8 for weights\n",
    "# \t\tactivation=torch.quantization.default_observer.with_args(dtype=torch.quint8)  # Use quint8 for activations\n",
    "# \t)\n",
    "\n",
    "# \tq_model = copy.deepcopy(model)\n",
    "\n",
    "# \tq_model.eval()\n",
    "# \t# 指定要融合的模块序列\n",
    "# \tmodules_to_fuse = [['features.0', 'features.1', 'features.2'],\n",
    "# \t\t\t\t\t['features.3', 'features.4', 'features.5'],\n",
    "# \t\t\t\t\t['features.7', 'features.8', 'features.9'],\n",
    "# \t\t\t\t\t['features.10', 'features.11', 'features.12'],\n",
    "# \t\t\t\t\t['features.14', 'features.15', 'features.16'],\n",
    "# \t\t\t\t\t['features.17', 'features.18', 'features.19'],\n",
    "# \t\t\t\t\t[\"classifier.0\",\"classifier.1\"]\n",
    "# \t\t\t\t\t]\n",
    "# \t# 融合模块\n",
    "# \t# q_model = torch.quantization.fuse_modules(q_model, modules_to_fuse)\n",
    "# \t# Prepare the model for quantization\n",
    "# \tq_model.qconfig = qconfig \n",
    "# \t# 只量化卷积层\n",
    "# \t# for n,m in q_model.named_modules():\n",
    "# \t# \tif isinstance(m,(nn.Conv2d,torch.quantization.QuantStub,torch.quantization.DeQuantStub)):\n",
    "# \t# \t\tm.qconfig = qconfig\n",
    "# \ttorch.quantization.prepare(q_model, inplace=True)\n",
    "# \t# Quantize the model\n",
    "# \t# run_model_on_data(model, data)\n",
    "# \tcalibra_data = torch.rand(shape)\n",
    "# \tq_model(calibra_data)\n",
    "# \ttorch.quantization.convert(q_model, inplace=True)\n",
    "# \treturn q_model\n",
    "\n",
    "# def fx_quantize_vgg(model,shape):\n",
    "# \tengine_name = 'qnnpack' # 'fbgemm' # 'qnnpack'\n",
    "# \tmodel_fp = copy.deepcopy(model)\n",
    "\n",
    "# \t# post training static quantization\n",
    "# \tmodel_to_quantize = copy.deepcopy(model_fp)\n",
    "# \t# fusion\n",
    "# \tmodel_to_quantize.eval()\n",
    "# \tmodel_to_quantize = quantize_fx.fuse_fx(model_to_quantize)\n",
    "# \tqconfig_mapping = get_default_qconfig_mapping(engine_name)\n",
    "# \texample_inputs = torch.rand(shape)\n",
    "# \t# prepare\n",
    "# \tmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
    "# \t# calibrate (not shown)\n",
    "# \tmodel_prepared(example_inputs)\n",
    "# \t# quantize\n",
    "# \tmodel_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "# \treturn model_quantized\n",
    "\n",
    "# def test_inference_speed(model,\n",
    "#                          input_shape,\n",
    "#                          qunantize_input=False,\n",
    "#                         test_iters=100,\n",
    "#                         verbose = True,\n",
    "#                         title = \"\"):\n",
    "#     quant = lambda x: torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8) # torch.quantization.QuantStub()\n",
    "#     dequant = torch.dequantize\n",
    "#     ecalps_time = 0\n",
    "#     batch_size = input_shape[0]\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # for images, labels in testloader:\n",
    "#         for i in tqdm(range(test_iters)):\n",
    "#             images = torch.rand(*input_shape)\n",
    "#             if qunantize_input:\n",
    "#                 images = quant(images)\n",
    "#             start_time = time.time()\n",
    "#             outputs = model(images)\n",
    "#             m = outputs.mean().item()\n",
    "#             ecalps_time += time.time()-start_time\n",
    "#     if verbose:\n",
    "#           print(f'{title} Inference time: {ecalps_time} seconds, FPS: {batch_size*test_iters/ecalps_time}')\n",
    "#     return ecalps_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 搜索所有可以融合的模块列表，Conv|Linear+[BN]+ReLU\n",
    "# import sys, os\n",
    "# notebook_path = os.getcwd()  # Get the current working directory\n",
    "# parent_directory = os.path.dirname(notebook_path)  # Get the parent directory\n",
    "# sys.path.append(parent_directory)\n",
    "# from models.conv import DirectConv2d,PadConv2d\n",
    "# def find_fuse_modules(model,parent_name=\"\"):\n",
    "# \tpre_conv2d=False\n",
    "# \tpre_linear=False\n",
    "# \tpointer_module = None\n",
    "# \tpointer_name = \"\"\n",
    "# \tnext = 0\n",
    "# \tmodules_to_fuse = []\n",
    "# \ttmp_modules_to_fuse = []\n",
    "# \tfor name, module in model.named_children():\n",
    "# \t\tmodule_name = f\"{parent_name}.{name}\" if parent_name!=\"\" else name\n",
    "# \t\tif isinstance(module, (nn.Linear, nn.Conv2d, DirectConv2d,PadConv2d)):\n",
    "# \t\t\tif isinstance(module, nn.Linear):\n",
    "# \t\t\t\tpre_linear = True\n",
    "# \t\t\telse:\n",
    "# \t\t\t\tpre_conv2d = True\n",
    "# \t\t\tpointer_module = module\n",
    "# \t\t\tpointer_name = name\n",
    "# \t\t\tnext = 0\n",
    "# \t\t\t# module_name = f\"{parent_name}.{name}\" if parent_name!=\"\" else name\n",
    "# \t\t\ttmp_modules_to_fuse.append(module_name)\n",
    "\t\t\t\n",
    "# \t\telif isinstance(module, (nn.BatchNorm2d,)):\n",
    "# \t\t\tif pre_conv2d or pre_linear:\n",
    "# \t\t\t\tnext+=1\n",
    "# \t\t\t\t# module_name = f\"{parent_name}.{name}\" if parent_name!=\"\" else name\n",
    "# \t\t\t\ttmp_modules_to_fuse.append(module_name)\n",
    "# \t\telif isinstance(module, (nn.ReLU, )):\n",
    "# \t\t\tif pre_conv2d or pre_linear:\n",
    "# \t\t\t\tnext+=1\n",
    "# \t\t\t\t# module_name = f\"{parent_name}.{name}\" if parent_name!=\"\" else name\n",
    "# \t\t\t\ttmp_modules_to_fuse.append(module_name)\n",
    "# \t\t\t\tmodules_to_fuse.append(tmp_modules_to_fuse)\n",
    "# \t\t\t\ttmp_modules_to_fuse = []\n",
    "# \t\t\t\tpre_linear = False\n",
    "# \t\t\t\tpre_conv2d = False\n",
    "# \t\t\t\tpointer_module = None\n",
    "# \t\t\t\tpointer_name = \"\"\n",
    "# \t\t\t\tnext = 0\n",
    "\t\t\t\t\n",
    "# \t\telse:\n",
    "# \t\t\tif pre_conv2d or pre_linear:\n",
    "# \t\t\t\tmodules_to_fuse.append(tmp_modules_to_fuse)\n",
    "# \t\t\t\t# if next==1 and pointer_module is not None:\n",
    "# \t\t\t\t# \tsetattr(model,pointer_name,FakeFuseModuleBN(pointer_module))\n",
    "# \t\t\tpre_linear = False\n",
    "# \t\t\tpre_conv2d = False\n",
    "# \t\t\tpointer_module = None\n",
    "# \t\t\tpointer_name = \"\"\n",
    "# \t\t\tnext = 0\n",
    "# \t\t\ttmp_modules_to_fuse = []\n",
    "# \t\t\t# module_name = f\"{parent_name}.{name}\" if parent_name!=\"\" else name\n",
    "# \t\t\tmodules_to_fuse += find_fuse_modules(module,module_name)\n",
    "\n",
    "# \treturn modules_to_fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['features.0', 'features.1', 'features.2'],\n",
       " ['features.3', 'features.4', 'features.5'],\n",
       " ['features.7', 'features.8', 'features.9'],\n",
       " ['features.10', 'features.11', 'features.12'],\n",
       " ['features.14', 'features.15', 'features.16'],\n",
       " ['features.17', 'features.18', 'features.19'],\n",
       " ['classifier.0', 'classifier.1']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取一个VGGmodel\n",
    "import torch\n",
    "from torch import nn\n",
    "class VGG7(nn.Module):\n",
    "    def __init__(self,base_inchannels = 64):\n",
    "        super(VGG7, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        C = base_inchannels\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, C, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(C),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(C, C, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(C),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(C, 2*C, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(2*C),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(2*C, 2*C, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(2*C),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(2*C, 4*C, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(4*C),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(4*C, 4*C, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(4*C),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # Global Average Pooling\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(4*C, 4*C),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4*C, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dequant(x)\n",
    "        x = self.classifier(x)\n",
    "        # self.quant()\n",
    "        return x\n",
    "\n",
    "find_fuse_modules(VGG7(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG7(\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VGG7(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9894e-01,  5.9960e-01, -8.4103e-01, -2.5476e-01,  5.8460e-01,\n",
       "         -6.2884e-01,  8.4536e-02,  3.9516e-01,  2.2429e-01, -2.0776e-02,\n",
       "         -1.5880e-01, -8.8674e-01, -2.1590e-03, -5.6504e-01,  6.4214e-01,\n",
       "         -1.5023e-01,  4.7914e-01, -4.2196e-01,  1.7772e-01,  3.6359e-01,\n",
       "          1.1824e+00, -6.9299e-01, -6.7443e-02,  3.7610e-01, -6.2179e-01,\n",
       "          1.4421e-03, -4.5854e-01,  2.5216e-01,  5.1884e-01,  3.2356e-01,\n",
       "         -3.4776e-01,  3.5834e-01,  4.2120e-01, -7.6882e-01, -9.6645e-02,\n",
       "          7.5362e-01,  1.0800e+00, -1.0191e-01,  2.7283e-01,  4.8794e-04,\n",
       "          3.6986e-01, -2.8406e-01, -6.5788e-01, -2.9301e-01, -2.4342e-01,\n",
       "          8.3793e-03, -3.1138e-01, -7.9395e-01,  6.9127e-01,  4.8942e-01,\n",
       "          4.8002e-01,  5.2548e-01, -9.6945e-01, -3.6319e-01,  1.1178e+00,\n",
       "         -1.0660e-01,  1.4564e-01,  4.4611e-01, -8.1166e-01, -3.1801e-01,\n",
       "          5.5797e-01, -9.4206e-02,  1.3277e+00, -2.9475e-01,  4.5796e-02,\n",
       "          1.0006e-01, -4.7705e-02,  1.2131e+00,  6.5703e-02, -3.3882e-01,\n",
       "          5.0870e-01,  9.4112e-01, -1.0960e-01,  1.5276e-01,  2.9419e-01,\n",
       "          6.6554e-02,  7.5392e-01,  9.1583e-01,  3.3199e-01, -3.0402e-01,\n",
       "         -2.0221e-01,  3.7842e-01,  4.5829e-01,  3.5182e-01,  7.5689e-01,\n",
       "         -4.7612e-01,  5.9758e-01,  3.8160e-01, -5.4570e-01, -1.3586e-01,\n",
       "         -1.7189e-03, -3.9769e-01,  2.0083e-01,  9.0091e-02,  2.5517e-01,\n",
       "         -8.7846e-01,  1.4707e-01,  3.0445e-01, -5.3001e-01, -2.9866e-01,\n",
       "          2.6078e-01, -1.7132e-01,  4.1510e-01,  2.5914e-01,  4.9583e-01,\n",
       "          3.6122e-01, -5.7811e-02,  6.9453e-02, -2.6705e-01, -5.0785e-01,\n",
       "         -5.1753e-01,  6.4111e-01,  9.0614e-01, -3.6813e-01, -3.1051e-01,\n",
       "         -6.3884e-01,  3.6623e-01,  1.3651e-01,  2.8507e-01,  4.7368e-01,\n",
       "          2.9891e-01,  4.6933e-02,  3.0126e-01,  2.6072e-01, -2.9424e-02,\n",
       "         -6.9408e-01, -5.6116e-01,  6.8310e-01, -2.4624e-01,  4.9015e-01,\n",
       "          6.3970e-01,  1.0343e+00,  1.7499e-01,  3.6237e-01, -4.8206e-02,\n",
       "         -3.2596e-01, -2.4404e-01,  6.7072e-02, -3.0745e-01, -2.7604e-01,\n",
       "         -2.1101e-02, -2.2677e-01, -2.0306e-01,  4.0898e-01, -2.8427e-01,\n",
       "         -3.5550e-01,  6.4243e-01, -7.0167e-01, -2.6337e-02, -5.4227e-01,\n",
       "          1.5101e-01, -1.2762e-01,  3.2530e-01,  1.2670e+00, -1.7878e-01,\n",
       "          1.0056e-01,  5.6970e-01, -1.9814e-01, -5.9644e-02,  4.1459e-01,\n",
       "          2.1277e-01, -4.2624e-01, -5.1229e-01, -4.4853e-01,  1.3711e-01,\n",
       "         -1.2303e-01,  1.1378e-01,  6.9559e-01, -1.0023e-01,  3.2906e-01,\n",
       "          2.4290e-01, -5.7354e-01, -5.4039e-01, -5.5391e-01,  3.9976e-01,\n",
       "         -7.2117e-02,  5.5099e-01, -4.8588e-01, -4.7957e-01,  3.8034e-01,\n",
       "          5.6651e-02, -7.4571e-01,  5.4776e-01,  4.6644e-01, -3.4105e-01,\n",
       "         -6.9857e-02, -5.2810e-01,  2.8208e-01, -6.7986e-02,  1.2101e-01,\n",
       "          4.2554e-01, -1.5330e-01, -5.2155e-01, -1.7732e-01, -6.0535e-01,\n",
       "          8.7144e-01,  1.1191e+00,  2.3348e-01,  8.7012e-01,  4.3816e-01,\n",
       "         -6.2172e-01,  6.6563e-01, -2.2446e-01,  2.3856e-01, -1.9520e-01,\n",
       "          1.7452e-01, -3.7116e-01, -5.3927e-01, -3.0175e-01,  2.1015e-01,\n",
       "          6.7639e-01,  2.7753e-01,  6.0218e-01, -1.3935e-01, -2.8862e-01,\n",
       "         -1.5127e+00, -2.9601e-01, -3.9687e-01, -4.2164e-01,  2.9249e-01,\n",
       "          1.1335e-02, -4.4516e-01, -1.4484e-01, -2.3177e-01, -3.1895e-01,\n",
       "          3.0684e-01,  7.0072e-01, -8.1499e-02,  1.9819e-01,  3.5389e-01,\n",
       "          7.7690e-01, -1.7511e-01, -5.0215e-02, -6.9049e-02,  3.5351e-03,\n",
       "         -1.8223e-01,  8.1115e-01, -1.3825e+00,  3.8732e-01, -2.1519e-01,\n",
       "         -1.0199e-02, -4.9589e-01,  1.2113e-01,  1.9206e-01, -4.4046e-01,\n",
       "         -8.3409e-02,  2.9052e-01,  7.8697e-02, -3.6155e-01, -9.1585e-01,\n",
       "          1.5973e-01, -1.0241e+00, -7.3203e-01, -8.8693e-01, -5.9881e-01,\n",
       "          5.9373e-01,  3.0892e-01,  2.8584e-02,  1.7883e-01, -3.7257e-01,\n",
       "         -1.7696e-01,  6.1695e-01,  6.8880e-02,  3.9711e-01, -2.6438e-01,\n",
       "         -4.0305e-01,  7.2536e-02, -4.3233e-02,  2.1997e-01, -1.4833e-01,\n",
       "          1.2431e-01, -6.2596e-01, -1.2437e-01, -3.7455e-01, -1.1591e+00,\n",
       "          5.8730e-01, -8.8843e-01,  8.1812e-02, -3.4986e-01,  3.9637e-02,\n",
       "         -1.6880e-01,  6.8979e-01, -6.2288e-01,  2.8316e-01, -7.9069e-02,\n",
       "          1.2775e-01, -2.0860e-01,  2.4071e-01, -5.1734e-01,  4.4397e-01,\n",
       "         -1.4412e-01,  3.1136e-01,  9.9446e-02,  1.6806e-01, -3.5742e-01,\n",
       "         -5.3368e-01, -1.7743e-01, -1.2468e-01,  6.8348e-01, -5.1893e-01,\n",
       "         -6.1208e-01, -6.3858e-02, -8.2311e-01,  9.6235e-02, -5.3129e-01,\n",
       "          6.6450e-01, -2.0706e-01, -3.9266e-01, -4.2273e-01,  7.9320e-01,\n",
       "         -6.0398e-01,  6.2045e-01,  1.0773e-01,  5.5935e-02,  7.4718e-02,\n",
       "          2.5045e-01,  4.5054e-01,  4.1876e-01,  1.2432e-01, -7.7523e-01,\n",
       "          1.2287e-02, -3.9046e-01,  2.6481e-01,  1.0633e+00, -6.3135e-01,\n",
       "          3.3932e-02, -4.5920e-01,  6.1934e-01, -8.8617e-02, -5.9049e-02,\n",
       "          9.8479e-01, -5.2777e-01, -1.0448e+00,  1.1812e-01,  2.4872e-01,\n",
       "         -2.2071e-01,  2.4504e-01,  1.2004e+00,  3.2163e-01,  7.9451e-01,\n",
       "          2.4813e-01,  3.7101e-03, -5.2242e-01, -4.7021e-01,  5.6585e-01,\n",
       "          3.6204e-02, -2.4386e-01, -1.3245e-01, -4.0468e-01,  2.7609e-01,\n",
       "          2.4564e-01, -1.3052e-01,  2.0524e-01,  4.2153e-01, -4.8488e-01,\n",
       "          4.0031e-01, -9.1409e-02,  2.3160e-01, -2.1067e-01,  1.3638e-01,\n",
       "          2.2489e-01, -1.9522e-01,  4.9480e-01,  6.1507e-01,  6.6405e-01,\n",
       "          4.9592e-01, -4.9185e-01,  2.7720e-01, -4.7318e-02,  1.1977e-01,\n",
       "         -4.4786e-01, -1.4372e-01, -7.0724e-01,  2.2213e-01,  2.9248e-01,\n",
       "         -3.5907e-01, -3.0847e-03,  4.9621e-02,  8.0526e-02, -4.6230e-02,\n",
       "          1.7268e-01,  7.7487e-02, -1.9584e-01,  3.4939e-01, -3.7882e-02,\n",
       "          4.7667e-02, -8.6297e-02,  2.7619e-01,  1.1398e-01,  6.2887e-01,\n",
       "          1.4040e-01,  9.7251e-02,  2.6296e-01, -5.9044e-01, -1.6572e-01,\n",
       "          4.8818e-01, -8.6769e-01, -2.9340e-01,  2.7856e-01,  1.5794e-01,\n",
       "          8.1779e-01,  3.2237e-01, -5.5703e-01,  2.9889e-01,  1.7480e-01,\n",
       "          1.0287e-01,  6.0019e-01,  1.5376e-01, -4.7630e-01, -5.1748e-03,\n",
       "         -2.3662e-01,  2.7442e-01, -6.4118e-01,  6.2861e-01, -1.8903e-01,\n",
       "         -3.5195e-01, -6.2306e-01,  2.6144e-01, -2.9161e-01, -1.8206e-01,\n",
       "         -5.6569e-01,  1.4969e-01,  5.3699e-01,  1.4197e-01, -1.5256e-02,\n",
       "          1.5567e-01,  9.0478e-01,  3.2341e-01,  4.2827e-01,  4.1803e-02,\n",
       "         -4.5290e-01, -1.1808e-01, -2.3073e-01,  4.7936e-01,  9.7414e-01,\n",
       "         -1.3268e-01,  6.0077e-01,  4.5726e-01, -7.0041e-01,  6.0445e-01,\n",
       "          9.4656e-03,  2.6389e-01, -1.7461e-01, -1.1785e-01,  2.1053e-02,\n",
       "          7.9312e-01, -8.0064e-01, -2.4857e-01,  5.9277e-01, -4.4840e-01,\n",
       "         -2.4508e-01, -1.8908e-01, -3.7561e-01,  1.2869e-02, -5.6840e-01,\n",
       "         -4.7219e-01, -7.4344e-01, -6.9957e-01, -1.0364e+00, -4.7052e-01,\n",
       "         -5.9907e-02, -5.5819e-01, -1.3719e+00, -3.8207e-01, -6.5331e-01,\n",
       "          2.6125e-01, -2.9148e-01,  9.7230e-02,  3.1244e-01,  5.9111e-01,\n",
       "          2.0629e-01, -1.8456e-01,  3.8155e-02, -9.3337e-02, -3.5343e-01,\n",
       "          5.0403e-01, -9.1192e-02,  3.1047e-01, -1.5123e-02,  4.4717e-01,\n",
       "          2.8958e-01,  5.5254e-01,  3.4091e-01, -5.6379e-02, -5.2546e-01,\n",
       "          5.6684e-01,  9.7721e-02,  4.8804e-01,  3.5741e-01, -4.7938e-02,\n",
       "          2.7872e-01, -2.6969e-01, -9.2431e-02,  4.6063e-01, -3.3927e-01,\n",
       "          2.3945e-01,  2.7500e-01,  7.0439e-02, -2.4608e-01, -2.9823e-01,\n",
       "          3.2717e-01, -1.5623e-01, -2.5155e-01,  3.9323e-01, -2.4240e-01,\n",
       "         -2.0502e-01, -2.3971e-01,  4.5557e-01, -1.0146e+00,  4.1868e-01,\n",
       "         -3.0340e-01, -3.1276e-01,  4.1143e-01, -4.4547e-01, -4.6792e-01,\n",
       "          7.1457e-01,  1.3392e-01,  1.1356e+00, -8.3987e-01, -9.2544e-01,\n",
       "          8.6266e-01,  6.0628e-01,  2.6991e-01,  1.0379e-01,  1.1370e+00,\n",
       "          9.8568e-01,  1.1135e-01, -4.1857e-01, -5.9400e-01,  8.6044e-01,\n",
       "         -1.1851e-01,  7.0092e-01,  3.6910e-01, -3.7780e-01, -1.0182e-01,\n",
       "          4.3378e-01,  1.3986e-01,  5.4034e-01,  1.2818e-01,  2.1650e-02,\n",
       "         -4.3172e-01,  1.6362e-01,  6.7495e-02,  2.3036e-01, -1.4198e-01,\n",
       "         -3.9436e-01,  3.5210e-01, -6.7155e-01,  4.1880e-01, -3.8017e-01,\n",
       "         -5.2971e-02,  2.9783e-01, -4.3999e-02,  2.2562e-01,  1.2482e-01,\n",
       "         -3.2552e-01,  2.9691e-01, -6.8716e-01,  4.9785e-01, -3.2245e-01,\n",
       "         -3.0460e-02, -1.0229e-01,  3.9446e-01,  4.1833e-01, -7.2226e-01,\n",
       "          3.8196e-01, -1.2859e-01,  1.9941e-01, -2.3910e-01,  5.9830e-01,\n",
       "          6.1066e-01, -5.7695e-01,  1.2330e-01,  5.3696e-02, -1.5049e-01,\n",
       "         -1.9915e-01, -1.9715e-01,  7.6101e-01, -1.1242e-01, -2.9437e-01,\n",
       "         -5.5649e-01, -1.1424e-01,  2.0601e-02, -7.8739e-01,  1.0072e-01,\n",
       "          2.7009e-01,  1.2587e-01, -3.7308e-01, -4.1556e-01, -4.7170e-01,\n",
       "         -3.7418e-01,  1.0551e-01,  3.2589e-01, -7.9879e-01,  2.8978e-01,\n",
       "          6.4873e-01,  3.9501e-01, -2.1966e-01, -3.2140e-01, -7.6366e-03,\n",
       "         -7.2240e-02, -8.1295e-01,  5.4418e-01,  7.2184e-02,  1.3877e-01,\n",
       "         -5.0551e-02,  4.9503e-01, -4.4438e-01, -1.3102e-01,  5.6398e-01,\n",
       "         -4.4097e-01, -1.0921e-01,  6.5690e-02, -1.0723e-02,  1.5796e-01,\n",
       "         -4.3957e-01,  7.3279e-03, -2.0276e-01,  9.6020e-01,  6.5736e-01,\n",
       "          5.8432e-01,  2.3559e-02, -4.9895e-01,  2.4448e-02, -8.2766e-01,\n",
       "          4.8538e-01,  6.4610e-01,  2.4862e-01, -1.4381e-01,  5.0622e-01,\n",
       "         -1.9088e-01,  7.1580e-02,  5.5046e-01, -2.6064e-01,  5.1753e-01,\n",
       "          4.9994e-01, -4.5605e-01, -6.8923e-01,  2.8574e-01, -2.1920e-01,\n",
       "         -8.3763e-01,  2.3817e-01,  6.9819e-01, -9.4258e-02, -3.5203e-01,\n",
       "          8.5883e-01,  3.7339e-01,  3.2934e-01,  1.2559e-01, -7.2300e-01,\n",
       "         -6.9291e-02,  7.2506e-02, -9.8506e-01,  6.6242e-01, -6.3732e-02,\n",
       "          8.6579e-01, -2.2074e-01,  8.3465e-01, -3.6793e-01, -1.6818e-01,\n",
       "          3.9882e-01, -7.7204e-01, -4.8869e-01, -2.8382e-04,  1.2710e-01,\n",
       "          2.2091e-01, -4.1945e-02, -7.2849e-01, -6.4003e-01,  2.7904e-01,\n",
       "          1.3826e-01,  2.4265e-01,  1.8904e-01, -4.4527e-01, -1.5680e-01,\n",
       "         -1.1424e+00,  5.0790e-01, -6.0925e-01,  2.1651e-01, -3.4546e-01,\n",
       "          5.8570e-01, -1.1045e-01, -2.4474e-01,  1.0343e-01, -6.9100e-03,\n",
       "         -7.9646e-01,  6.1164e-01,  3.8636e-01, -2.2780e-01, -2.1548e-01,\n",
       "          2.2365e-01,  1.1504e-01, -5.1023e-01, -5.6193e-01,  1.1582e+00,\n",
       "         -8.0644e-01,  1.0502e+00,  9.5223e-02,  1.2248e+00, -6.9375e-01,\n",
       "         -7.4140e-02,  7.3982e-01, -1.3822e-01,  1.9045e-02, -3.5434e-01,\n",
       "         -1.5423e-01, -1.5434e-01,  1.9967e-01, -3.5823e-01,  6.2126e-02,\n",
       "         -4.9901e-02, -4.6675e-01, -5.6266e-03, -2.2686e-02, -1.0223e+00,\n",
       "         -5.2409e-02, -2.5224e-01,  4.7399e-02, -7.3902e-02,  3.4490e-01,\n",
       "         -2.0540e-01, -1.0743e-04, -1.9936e-01, -8.7509e-02, -3.0632e-01,\n",
       "          1.6585e-01,  5.5379e-01,  1.3838e-01, -2.6575e-01,  9.2629e-02,\n",
       "         -3.3228e-01, -1.5150e-01,  2.4094e-01,  4.4344e-01, -1.6695e-01,\n",
       "         -4.6622e-01,  9.2883e-02,  1.6365e-01, -4.6360e-03, -4.1795e-01,\n",
       "          7.8460e-01,  7.0981e-02,  1.0137e+00,  3.0124e-01, -6.5241e-01,\n",
       "         -2.2716e-01, -1.6748e-01,  2.7102e-01, -8.2034e-01,  1.2172e+00,\n",
       "         -1.4952e-01,  6.5737e-01,  6.5513e-01,  2.3417e-01,  5.5870e-01,\n",
       "          2.9362e-01, -4.4230e-02,  3.7785e-01, -6.1288e-01,  2.8345e-01,\n",
       "         -2.5324e-01, -2.2422e-01,  6.5512e-01,  1.0880e-01,  3.7746e-01,\n",
       "          6.9288e-01, -1.4283e-01,  1.4710e-01,  2.1061e-01, -4.3194e-01,\n",
       "         -5.4934e-01, -6.4142e-01,  4.0401e-01, -4.2085e-01,  1.2840e-01,\n",
       "         -8.8119e-01, -4.2561e-01,  1.0954e+00,  2.5309e-02,  9.8998e-01,\n",
       "         -4.1961e-01,  1.0095e-02,  7.0114e-02,  4.5928e-01,  5.3737e-01,\n",
       "         -4.7247e-01, -2.7081e-01, -3.0658e-01,  1.7402e-01,  8.5560e-01,\n",
       "         -3.4994e-01,  1.1291e+00,  3.3301e-01, -2.5260e-01,  6.1551e-01,\n",
       "          3.4972e-01, -5.8496e-01,  8.2726e-01,  4.4362e-01, -2.7148e-01,\n",
       "         -4.7601e-01, -6.7741e-01,  2.0834e-01,  7.3785e-01, -4.3916e-01,\n",
       "         -2.0932e-01, -2.1712e-01,  8.9913e-02, -7.4586e-02,  2.4272e-01,\n",
       "         -6.6873e-01,  7.3805e-01,  6.7909e-01,  2.5461e-01, -1.7538e-01,\n",
       "          1.8053e-01,  3.5570e-01, -3.0992e-02, -2.4799e-01, -1.3045e-01,\n",
       "         -2.9391e-01, -2.6559e-01, -5.7175e-01,  9.5646e-02, -3.4254e-01,\n",
       "          2.7776e-01,  1.5679e-01, -6.5366e-02,  6.7963e-01, -2.1657e-02,\n",
       "         -2.6098e-02, -9.3875e-01,  4.6273e-01,  1.6840e-01,  3.1867e-01,\n",
       "         -4.0855e-01, -2.0849e-01,  2.2395e-01,  4.3101e-01,  4.4683e-01,\n",
       "         -5.1478e-01,  7.4732e-02,  2.6960e-01, -6.6151e-02, -1.6795e-01,\n",
       "          6.7162e-01, -6.9583e-01,  7.8303e-01,  2.5658e-01,  5.7720e-01,\n",
       "         -3.6669e-01, -5.2823e-01, -2.2800e-01,  3.9554e-01, -8.3520e-01,\n",
       "         -4.6459e-01, -8.0236e-01, -3.6932e-01, -8.5965e-01,  1.2813e-01,\n",
       "         -3.0231e-01, -3.4934e-01,  4.0861e-01, -1.0682e+00,  2.2303e-01,\n",
       "         -4.7759e-02, -7.0920e-03,  3.8708e-02, -2.0987e-01,  3.4850e-01,\n",
       "          3.4948e-01,  2.5073e-01, -1.1167e-01,  3.7584e-01, -1.3297e-01,\n",
       "         -7.2302e-01, -4.9167e-02, -6.9598e-01,  3.6680e-01,  4.4772e-01,\n",
       "          5.5820e-01,  7.7661e-03, -4.7703e-01,  9.2635e-02, -5.3889e-01,\n",
       "         -3.4414e-01, -9.1839e-01, -1.4653e-01,  3.1075e-01, -1.8178e-01,\n",
       "         -6.9641e-01,  2.3514e-01,  6.3654e-01,  3.8706e-01,  4.5121e-01,\n",
       "          1.1902e-01, -2.9673e-01,  4.6574e-01,  6.4535e-01,  2.0178e-01,\n",
       "          3.1331e-01, -5.6496e-01,  1.4419e-01, -4.2883e-01, -2.0343e-01,\n",
       "          5.2138e-01, -2.9334e-01, -3.1142e-02,  4.9066e-01, -3.5898e-01,\n",
       "          3.0290e-01, -7.4464e-01,  5.5909e-02,  5.3302e-02,  6.3867e-01,\n",
       "         -6.8599e-01,  2.3499e-01,  2.9884e-01,  3.7916e-01,  7.9303e-01,\n",
       "          4.4594e-02, -1.0823e+00, -4.7456e-02, -3.1872e-01,  9.5338e-01,\n",
       "         -7.5893e-01, -5.8308e-02, -2.6518e-01,  6.5753e-01,  5.7575e-02,\n",
       "         -9.0085e-01, -1.1277e-01, -4.1770e-01,  2.8805e-01, -6.3746e-01,\n",
       "          2.3649e-01, -3.3819e-01,  5.3502e-01, -8.7344e-02,  8.0379e-02,\n",
       "         -1.1223e-01,  4.7023e-01,  4.6563e-01,  5.2830e-01, -7.9502e-01,\n",
       "          6.8583e-01,  1.4780e-01,  4.9008e-01,  1.0587e-01,  3.0919e-01,\n",
       "         -6.6071e-01, -4.6433e-02,  5.3122e-01, -2.6590e-01,  1.9681e-01,\n",
       "          4.8605e-01,  7.1087e-01, -5.7494e-01,  5.6626e-01, -2.6914e-01,\n",
       "          5.6166e-01, -5.4817e-02, -7.7403e-01,  1.1175e-01, -4.9075e-01,\n",
       "          5.8216e-01, -4.6212e-01,  9.3097e-02,  3.6340e-01, -7.2528e-01,\n",
       "         -2.3352e-01,  2.1066e-01, -4.8727e-02, -8.0732e-01,  5.8421e-01,\n",
       "         -4.7426e-01,  7.4480e-01, -6.6188e-01,  2.8676e-01,  5.4073e-01,\n",
       "         -3.5236e-01, -5.1001e-01,  8.2790e-01, -9.6470e-01,  6.4286e-01,\n",
       "         -3.0172e-01,  3.7636e-01, -7.9123e-01,  1.4932e-01, -3.4501e-01,\n",
       "          2.4382e-02, -5.9424e-01,  1.1806e+00,  4.7160e-01,  7.7799e-01,\n",
       "          9.4654e-02, -2.9250e-01,  1.5406e-01, -7.0560e-01,  7.0163e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入ResNet18\n",
    "# import torchvision.models as models\n",
    "# resnet18 = models.resnet18()\n",
    "# import torch\n",
    "# import sys, os\n",
    "# notebook_path = os.getcwd()  # Get the current working directory\n",
    "# parent_directory = os.path.dirname(notebook_path)  # Get the parent directory\n",
    "# sys.path.append(parent_directory)\n",
    "# from models.resnet import ResNet18\n",
    "# resnet18 = ResNet18()\n",
    "# convert_to_direct_conv2d(fake_fuse_module_bn_relu(resnet18),3,3, module_type=nn.Conv2d)\n",
    "resnet18(torch.rand(1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert module to PadConv2d\n",
      "convert conv2 to PadConv2d\n",
      "convert module to PadConv2d\n",
      "convert conv2 to PadConv2d\n",
      "convert module to PadConv2d\n",
      "convert module to PadConv2d\n",
      "convert conv2 to PadConv2d\n",
      "convert module to PadConv2d\n",
      "convert module to PadConv2d\n",
      "convert conv2 to PadConv2d\n",
      "convert module to PadConv2d\n",
      "convert module to PadConv2d\n",
      "convert conv2 to PadConv2d\n",
      "convert module to DirectConv2d\n",
      "convert conv2 to DirectConv2d\n",
      "convert module to DirectConv2d\n",
      "convert conv2 to DirectConv2d\n",
      "convert module to DirectConv2d\n",
      "convert module to DirectConv2d\n",
      "convert conv2 to DirectConv2d\n",
      "convert module to DirectConv2d\n",
      "convert module to DirectConv2d\n",
      "convert conv2 to DirectConv2d\n",
      "convert module to DirectConv2d\n",
      "convert module to DirectConv2d\n",
      "convert conv2 to DirectConv2d\n"
     ]
    }
   ],
   "source": [
    "# module_func = lambda : VGG7(64)\n",
    "module_func = models.resnet18\n",
    "model = convert_to_pad_conv2d(fake_fuse_module_bn_relu(module_func()), 3, 3, module_type=nn.Conv2d, verbose=True)\n",
    "mymodel = convert_to_direct_conv2d(fake_fuse_module_bn_relu(module_func()),3,3,module_type=nn.Conv2d,verbose=True)\n",
    "shape = (3,224,224)\n",
    "batch_size = 4\n",
    "calibra_shape = (1,*shape)\n",
    "input_shape = (batch_size,*shape)\n",
    "# eager quantize\n",
    "eager_quant_model = eager_quantize_vgg(model, calibra_shape)\n",
    "\n",
    "# fx quantize\n",
    "# fx_quant_model = fx_quantize_vgg(model, calibra_shape)\n",
    "# test_inference_speed(model,\n",
    "# \t\t\t\t\t input_shape,\n",
    "# \t\t\t\t\t qunantize_input=False, \n",
    "# \t\t\t\t\t verbose=True,\n",
    "# \t\t\t\t\t title=\"Float\")\n",
    "# test_inference_speed(eager_quant_model, \n",
    "# \t\t\t\t\t input_shape, \n",
    "# \t\t\t\t\t qunantize_input=True, \n",
    "# \t\t\t\t\t verbose=True,\n",
    "# \t\t\t\t\t title=\"Eager Quantized\")\n",
    "# test_inference_speed(fx_quant_model,\n",
    "# \t\t\t\t\t input_shape,\n",
    "# \t\t\t\t\t qunantize_input=False,\n",
    "# \t\t\t\t\t verbose=True,\n",
    "# \t\t\t\t\t title=\"FX Quantized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100%|██████████| 100/100 [00:41<00:00,  2.42it/s]\n",
    "Float Inference time: 40.943278074264526 seconds, FPS: 39.07845378422942\n",
    "100%|██████████| 100/100 [00:22<00:00,  4.51it/s]\n",
    "Eager Quantized Inference time: 21.75051259994507 seconds, FPS: 73.56148470744735\n",
    "100%|██████████| 100/100 [00:22<00:00,  4.49it/s]\n",
    "FX Quantized Inference time: 21.84299397468567 seconds, FPS: 73.25003165107657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG7(\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       "  (features): Sequential(\n",
       "    (0): FakeFuseModuleBNReLU(\n",
       "      (module): PadConv2d(\n",
       "        (conv): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (1): Identity()\n",
       "    (2): Identity()\n",
       "    (3): FakeFuseModuleBNReLU(\n",
       "      (module): PadConv2d(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (4): Identity()\n",
       "    (5): Identity()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): FakeFuseModuleBNReLU(\n",
       "      (module): PadConv2d(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (8): Identity()\n",
       "    (9): Identity()\n",
       "    (10): FakeFuseModuleBNReLU(\n",
       "      (module): PadConv2d(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (11): Identity()\n",
       "    (12): Identity()\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): FakeFuseModuleBNReLU(\n",
       "      (module): PadConv2d(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (15): Identity()\n",
       "    (16): Identity()\n",
       "    (17): FakeFuseModuleBNReLU(\n",
       "      (module): PadConv2d(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (18): Identity()\n",
       "    (19): Identity()\n",
       "    (20): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): FakeFuseModuleReLU(\n",
       "      (module): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): Identity()\n",
       "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dconv2d_model = copy.deepcopy(model)\n",
    "# dconv2d_model = convert_to_direct_conv2d(dconv2d_model, 3, 3, \n",
    "# \t\t\t\t\t\t\t\t\t\t module_type_name=type(model.features[0]).__name__,\n",
    "# \t\t\t\t\t\t\t\t\t\t prepare_func=lambda x: x.int())\n",
    "# quant = lambda x: torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "# # dconv2d_model.features[17].post_func = quant\n",
    "# eager_quant_model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float Inference time: 16.892439126968384 seconds, FPS: 2.367923288007647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.892439126968384"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dconv2d_model(x)\n",
    "test_inference_speed(model,\n",
    "\t\t\t\t\t input_shape,\n",
    "\t\t\t\t\t test_iters = 10,\n",
    "\t\t\t\t\t qunantize_input=False,\n",
    "\t\t\t\t\t verbose=True,\n",
    "\t\t\t\t\t title=\"Float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Conv2d Inference time: 12.24937391281128 seconds, FPS: 3.265473018026261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.24937391281128"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dconv2d_model(x)\n",
    "test_inference_speed(mymodel,\n",
    "\t\t\t\t\t input_shape,\n",
    "\t\t\t\t\t test_iters = 10,\n",
    "\t\t\t\t\t qunantize_input=False,\n",
    "\t\t\t\t\t verbose=True,\n",
    "\t\t\t\t\t title=\"Direct Conv2d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::add.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:31357 [kernel]\nMeta: registered at /dev/null:228 [kernel]\nMkldnnCPU: registered at /pytorch/build/aten/src/ATen/RegisterMkldnnCPU.cpp:515 [kernel]\nSparseCPU: registered at /pytorch/build/aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCsrCPU: registered at /pytorch/build/aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at /pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:21977 [kernel]\nNamed: fallthrough registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4832 [kernel]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradHIP: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradMPS: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradIPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradXPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradHPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradVE: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradLazy: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradMTIA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradMeta: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradNestedTensor: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:17346 [kernel]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[176], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# dconv2d_model(x)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_inference_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[43meager_quant_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m\t\t\t\t\t \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m\t\t\t\t\t \u001b[49m\u001b[43mtest_iters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m\t\t\t\t\t \u001b[49m\u001b[43mqunantize_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m\t\t\t\t\t \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m\t\t\t\t\t \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEager int8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[174], line 94\u001b[0m, in \u001b[0;36mtest_inference_speed\u001b[0;34m(model, input_shape, qunantize_input, test_iters, verbose, title)\u001b[0m\n\u001b[1;32m     92\u001b[0m     images \u001b[38;5;241m=\u001b[39m quant(images)\n\u001b[1;32m     93\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 94\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m m \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     96\u001b[0m ecalps_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart_time\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m--> 273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/resnet.py:102\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[0;32m--> 102\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[1;32m    103\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::add.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, Meta, MkldnnCPU, SparseCPU, SparseCsrCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:31357 [kernel]\nMeta: registered at /dev/null:228 [kernel]\nMkldnnCPU: registered at /pytorch/build/aten/src/ATen/RegisterMkldnnCPU.cpp:515 [kernel]\nSparseCPU: registered at /pytorch/build/aten/src/ATen/RegisterSparseCPU.cpp:1387 [kernel]\nSparseCsrCPU: registered at /pytorch/build/aten/src/ATen/RegisterSparseCsrCPU.cpp:1135 [kernel]\nBackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at /pytorch/build/aten/src/ATen/RegisterFunctionalization_0.cpp:21977 [kernel]\nNamed: fallthrough registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /pytorch/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4832 [kernel]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradHIP: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradMPS: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradIPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradXPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradHPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradVE: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradLazy: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradMTIA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradMeta: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nAutogradNestedTensor: registered at /pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:17434 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_2.cpp:17346 [kernel]\nAutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "# dconv2d_model(x)\n",
    "test_inference_speed(eager_quant_model,\n",
    "\t\t\t\t\t input_shape,\n",
    "\t\t\t\t\t test_iters = 10,\n",
    "\t\t\t\t\t qunantize_input=True,\n",
    "\t\t\t\t\t verbose=True,\n",
    "\t\t\t\t\t title=\"Eager int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ResNet' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[178], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tmp_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m*\u001b[39minput_shape)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmymodel\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      3\u001b[0m \tstart \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \ttmp_inputs \u001b[38;5;241m=\u001b[39m layer(tmp_inputs)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ResNet' object is not iterable"
     ]
    }
   ],
   "source": [
    "tmp_inputs = torch.rand(*input_shape)\n",
    "for i, layer in enumerate(mymodel.features):\n",
    "\tstart = time.time()\n",
    "\ttmp_inputs = layer(tmp_inputs)\n",
    "\tprint(f\"layer {i} {tmp_inputs.shape} time: {(time.time()-start)*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 torch.Size([16, 128, 32, 32]) time: 18.08ms\n",
      "layer 1 torch.Size([16, 128, 32, 32]) time: 0.07ms\n",
      "layer 2 torch.Size([16, 128, 32, 32]) time: 0.02ms\n",
      "layer 3 torch.Size([16, 128, 32, 32]) time: 231.99ms\n",
      "layer 4 torch.Size([16, 128, 32, 32]) time: 0.04ms\n",
      "layer 5 torch.Size([16, 128, 32, 32]) time: 0.02ms\n",
      "layer 6 torch.Size([16, 128, 16, 16]) time: 7.54ms\n",
      "layer 7 torch.Size([16, 256, 16, 16]) time: 146.10ms\n",
      "layer 8 torch.Size([16, 256, 16, 16]) time: 0.04ms\n",
      "layer 9 torch.Size([16, 256, 16, 16]) time: 0.02ms\n",
      "layer 10 torch.Size([16, 256, 16, 16]) time: 295.80ms\n",
      "layer 11 torch.Size([16, 256, 16, 16]) time: 0.04ms\n",
      "layer 12 torch.Size([16, 256, 16, 16]) time: 0.02ms\n",
      "layer 13 torch.Size([16, 256, 8, 8]) time: 7.25ms\n",
      "layer 14 torch.Size([16, 512, 8, 8]) time: 231.41ms\n",
      "layer 15 torch.Size([16, 512, 8, 8]) time: 0.04ms\n",
      "layer 16 torch.Size([16, 512, 8, 8]) time: 0.02ms\n",
      "layer 17 torch.Size([16, 512, 8, 8]) time: 307.71ms\n",
      "layer 18 torch.Size([16, 512, 8, 8]) time: 0.04ms\n",
      "layer 19 torch.Size([16, 512, 8, 8]) time: 0.02ms\n",
      "layer 20 torch.Size([16, 512, 1, 1]) time: 6.75ms\n"
     ]
    }
   ],
   "source": [
    "tmp_inputs = quant(torch.rand(*input_shape))\n",
    "for i, layer in enumerate(eager_quant_model.features):\n",
    "\tstart = time.time()\n",
    "\ttmp_inputs = layer(tmp_inputs)\n",
    "\tprint(f\"layer {i} {tmp_inputs.shape} time: {(time.time()-start)*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer torch.Size([16, 128, 32, 32]) 0 time: 101.52ms\n",
      "layer torch.Size([16, 128, 32, 32]) 1 time: 20.16ms\n",
      "layer torch.Size([16, 128, 32, 32]) 2 time: 11.49ms\n",
      "layer torch.Size([16, 128, 32, 32]) 3 time: 635.39ms\n",
      "layer torch.Size([16, 128, 32, 32]) 4 time: 13.08ms\n",
      "layer torch.Size([16, 128, 32, 32]) 5 time: 15.57ms\n",
      "layer torch.Size([16, 128, 16, 16]) 6 time: 7.09ms\n",
      "layer torch.Size([16, 256, 16, 16]) 7 time: 254.70ms\n",
      "layer torch.Size([16, 256, 16, 16]) 8 time: 6.28ms\n",
      "layer torch.Size([16, 256, 16, 16]) 9 time: 4.09ms\n",
      "layer torch.Size([16, 256, 16, 16]) 10 time: 370.96ms\n",
      "layer torch.Size([16, 256, 16, 16]) 11 time: 5.85ms\n",
      "layer torch.Size([16, 256, 16, 16]) 12 time: 4.21ms\n",
      "layer torch.Size([16, 256, 8, 8]) 13 time: 7.13ms\n",
      "layer torch.Size([16, 512, 8, 8]) 14 time: 176.16ms\n",
      "layer torch.Size([16, 512, 8, 8]) 15 time: 2.66ms\n",
      "layer torch.Size([16, 512, 8, 8]) 16 time: 1.41ms\n",
      "layer torch.Size([16, 512, 8, 8]) 17 time: 419.73ms\n",
      "layer torch.Size([16, 512, 8, 8]) 18 time: 7.27ms\n",
      "layer torch.Size([16, 512, 8, 8]) 19 time: 1.98ms\n",
      "layer torch.Size([16, 512, 1, 1]) 20 time: 1.32ms\n"
     ]
    }
   ],
   "source": [
    "# tmp_inputs = quant(torch.rand(*input_shape))\n",
    "tmp_inputs = torch.rand(*input_shape)\n",
    "for i, layer in enumerate(model.features):\n",
    "\tstart = time.time()\n",
    "\ttmp_inputs = layer(tmp_inputs)\n",
    "\tprint(f\"layer {tmp_inputs.shape} {i} time: {(time.time()-start)*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-06-23 14:49:41 214608:214608 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference        79.17%     354.279ms       100.00%     447.510ms     447.510ms             1  \n",
      "                      aten::copy_         8.00%      35.799ms         8.00%      35.799ms       2.387ms            15  \n",
      "                        aten::pad         0.03%     116.000us         7.51%      33.630ms       5.605ms             6  \n",
      "            aten::constant_pad_nd         0.13%     580.000us         7.49%      33.514ms       5.586ms             6  \n",
      "                 aten::max_pool2d         0.01%      41.000us         6.31%      28.217ms      14.108ms             2  \n",
      "    aten::max_pool2d_with_indices         3.90%      17.461ms         6.30%      28.176ms      14.088ms             2  \n",
      "                      aten::fill_         4.28%      19.161ms         4.28%      19.161ms       3.193ms             6  \n",
      "                 aten::contiguous         0.01%      41.000us         2.95%      13.217ms       3.304ms             4  \n",
      "                      aten::clone         0.14%     605.000us         2.94%      13.176ms       3.294ms             4  \n",
      "                         aten::to         0.02%      96.000us         2.39%      10.692ms       1.782ms             6  \n",
      "                   aten::_to_copy         0.06%     267.000us         2.37%      10.596ms       1.766ms             6  \n",
      "                quantized::linear         1.79%       8.023ms         1.80%       8.062ms       8.062ms             1  \n",
      "        aten::adaptive_avg_pool2d         0.00%      18.000us         1.08%       4.827ms       4.827ms             1  \n",
      "       aten::_adaptive_avg_pool2d         0.94%       4.201ms         1.07%       4.809ms       4.809ms             1  \n",
      "           quantized::linear_relu         0.95%       4.267ms         0.96%       4.303ms       4.303ms             1  \n",
      "        aten::quantize_per_tensor         0.11%     492.000us         0.54%       2.415ms       2.415ms             1  \n",
      "                      aten::slice         0.18%     802.000us         0.23%       1.040ms      34.667us            30  \n",
      "                      aten::empty         0.11%     498.000us         0.11%     498.000us      33.200us            15  \n",
      "                     aten::narrow         0.04%     161.000us         0.10%     446.000us      74.333us             6  \n",
      "                 aten::as_strided         0.05%     238.000us         0.05%     238.000us       7.933us            30  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 447.510ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-06-23 14:49:41 214608:214608 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-06-23 14:49:41 214608:214608 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# model = models.resnet18().eval()  # 使用ResNet18作为示例模型\n",
    "inputs = torch.randn(16, 3, 32, 32)  # 创建一个示例输入\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        # with record_function(\"direct_conv2d_func\"):  # 显式地标记你的自定义函数\n",
    "            # eager_quant_model(quant(inputs))\n",
    "        dconv2d_model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float Inference time: 15.419025897979736 seconds, FPS: 10.376790405479756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.419025897979736"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ishape = eager_quant_model.features[0](quant(x)).shape\n",
    "test_inference_speed(model,\n",
    "\t\t\t\t\t [16, 3, 32, 32],\n",
    "\t\t\t\t\t#  test_iters = 10,\n",
    "\t\t\t\t\t qunantize_input=False,\n",
    "\t\t\t\t\t verbose=True,\n",
    "\t\t\t\t\t title=\"Float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:53<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager Quantized Inference time: 53.04732060432434 seconds, FPS: 30.161749580799192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53.04732060432434"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ishape = eager_quant_model.features[0](quant(x)).shape\n",
    "test_inference_speed(eager_quant_model.features[0],\n",
    "\t\t\t\t\t [16, 3, 32, 32],\n",
    "\t\t\t\t\t qunantize_input=True,\n",
    "\t\t\t\t\t verbose=True,\n",
    "\t\t\t\t\t title=\"Eager Quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 44.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Quantized Inference time: 2.1228220462799072 seconds, FPS: 753.7136722335651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.1228220462799072"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inference_speed(dconv2d_model.features[0],\n",
    "\t\t\t\t\t [16, 3, 32, 32],\n",
    "\t\t\t\t\t qunantize_input=False,\n",
    "\t\t\t\t\t verbose=True,\n",
    "\t\t\t\t\t title=\"Direct Quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  5.200766324996948\n",
      "FLOPS:  380507258880\n",
      "GFLOPS:  73.16369071441088\n",
      "FPS:  30.764696969940076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "W_bits = 3\n",
    "A_bits = 3\n",
    "max_w = 2**(W_bits) - 1\n",
    "max_a = 2**(A_bits) - 1\n",
    "N = 16\n",
    "Ci = 1024\n",
    "H = 7\n",
    "W = 12\n",
    "Co = 1024\n",
    "K = 3\n",
    "regA = 2\n",
    "TN =2\n",
    "if (W_bits+A_bits)<3:\n",
    "\tTN = 4\n",
    "align_num = regA*K*TN\n",
    "W = int((W//align_num)*align_num+((W%align_num)>0)*align_num) \n",
    "print(\"W: \", W)\n",
    "inp = torch.randint(0, max_a, (N, Ci, H, W)).int()\n",
    "weight = torch.randint(0, max_w, (Co, Ci, K,K)).int()\n",
    "\n",
    "conv = DirectConv2d(Ci, Co, K, W_bits, A_bits)\n",
    "# start = time.time()\n",
    "for i in range(4):\n",
    "\t# output = direct_conv2d.direct_conv2d(inp, weight, W_bits, A_bits,1,1,1,1)\n",
    "\tout = conv(inp)\n",
    "# escap_time = time.time() - start\n",
    "# start = time.time()\n",
    "output = direct_conv2d.direct_conv2d(inp, weight, W_bits, A_bits,1,0,1,1)\n",
    "# escap_time = time.time() - start\n",
    "from tqdm import tqdm\n",
    "ecalps_time = 0\n",
    "test_iters = 10\n",
    "for i in tqdm(range(test_iters)):\n",
    "\tinp = torch.randint(0, max_a, (N, Ci, H, W)).int()\n",
    "\tstart_time = time.time()\n",
    "\toutput = direct_conv2d.direct_conv2d(inp, weight, W_bits, A_bits,1,0,1,1)\n",
    "\t# out = conv(inp)\n",
    "\tecalps_time += time.time()-start_time\n",
    "\n",
    "_,_,Ho,Wo = output.shape\n",
    "print(\"Time taken: \", ecalps_time)\n",
    "flops = 2*N*Ci*Ho*Wo*Co*K*K*test_iters\n",
    "print(\"FLOPS: \", flops)\n",
    "print(\"GFLOPS: \", flops/ecalps_time/1e9)\n",
    "print(\"FPS: \", (N*test_iters)/ecalps_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 9, 14])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output = direct_conv2d.direct_conv2d(inp, weight, W_bits, A_bits,1,1,1,1)\n",
    "# output.shape\n",
    "# output_quant.shape\n",
    "# 2*N*Ci*H* W* Co*K*K*test_iters\n",
    "# 2*N*Ci*Ho*Wo*Co*K*K*test_iters\n",
    "# Ho,H,Wo,W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_ = plt.hist(output.numpy().flatten(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  18.856735706329346\n",
      "FLOPS:  380507258880\n",
      "GFLOPS:  20.17885093188643\n",
      "FPS:  8.485031687976372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import conv2d\n",
    "\n",
    "inp = torch.randint(0, max_a, (N, Ci, H, W)).float()\n",
    "weight = torch.randint(0, max_w, (Co, Ci, K,K)).float()\n",
    "\n",
    "escap_time = 0\n",
    "for i in tqdm(range(test_iters)):\n",
    "\tstart = time.time()\n",
    "\tfloat_output = conv2d(inp, weight, groups = 1, stride = 1, padding = 2, dilation = 1)\n",
    "\tescap_time += time.time() - start\n",
    "_,_,Ho,Wo = float_output.shape\n",
    "print(\"Time taken: \", escap_time)\n",
    "flops = 2*N*Ci*Ho*Wo*Co*K*K*test_iters\n",
    "print(\"FLOPS: \", flops)\n",
    "print(\"GFLOPS: \", flops/escap_time/1e9)\n",
    "print(\"FPS: \", (N*test_iters)/escap_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  10.207385301589966\n",
      "FLOPS:  380507258880\n",
      "GFLOPS:  37.27764237730203\n",
      "FPS:  15.674925093214362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch.nn.quantized.functional import conv2d\n",
    "\n",
    "# 假设N, Ci, H, W, Co, K, max_a, max_w, scale, zero_point已经定义\n",
    "inp = torch.randint(0, max_a, (N, Ci, H, W)).float()\n",
    "weight = torch.randint(0, max_w, (Co, Ci, K, K)).float()\n",
    "scale = torch.tensor(1.0)  # 实际应用中应通过量化校准过程得到\n",
    "zero_point = torch.tensor(0)  # 对于输入使用quint8，zero_point通常为0\n",
    "\n",
    "# 创建量化的输入和权重\n",
    "inp_quant = torch.quantize_per_tensor(inp, scale=scale, zero_point=zero_point, dtype=torch.quint8)\n",
    "weight_quant = torch.quantize_per_tensor(weight, scale=scale, zero_point=zero_point, dtype=torch.qint8)\n",
    "\n",
    "# 执行量化卷积操作\n",
    "elapsed_time = 0\n",
    "for i in tqdm(range(test_iters)):\n",
    "\tstart = time.time()\n",
    "\toutput_quant = conv2d(inp_quant, weight_quant, None, stride=1, padding=2, dilation=1, groups=1, scale=scale, zero_point=zero_point)\n",
    "\telapsed_time += time.time() - start\n",
    "\n",
    "_,_,Ho,Wo = output_quant.shape\n",
    "# 性能测量\n",
    "print(\"Time taken: \", elapsed_time)\n",
    "# 注意：这里的FLOPS计算需要根据实际的输出尺寸进行调整\n",
    "flops = 2 * N * Co * Ho * Wo * Ci * K * K*test_iters  # 这里假设输出尺寸与输入相同，实际上可能不同\n",
    "print(\"FLOPS: \", flops)\n",
    "print(\"GFLOPS: \", flops / elapsed_time / 1e9)\n",
    "print(\"FPS: \", (N*test_iters)/elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假设N, Ci, H, W, Co, K, max_a, max_w已经定义\n",
    "W_bits = A_bits = 4\n",
    "N = 16\n",
    "Ci = 64\n",
    "H = 224\n",
    "align_num = 12\n",
    "if W_bits + A_bits < 3:\n",
    "    align_num = 24\n",
    "W = H//align_num*align_num+((H%align_num)>0)*align_num\n",
    "Co = 64\n",
    "K = 3\n",
    "# Flops = 2 * N * Co * H * W * Ci * K * K\n",
    "\n",
    "# 准备输入和权重\n",
    "# inp = torch.randint(0, max_a, (N, Ci, H, W)).float()\n",
    "# weight = torch.randint(0, max_w, (Co, Ci, K, K)).float()\n",
    "# inp = torch.rand(N, Ci, H, W)\n",
    "weight = torch.rand(Co, Ci, K, K)\n",
    "\n",
    "# 量化准备\n",
    "# quant = QuantStub()\n",
    "# dequant = DeQuantStub()\n",
    "quant = lambda x: torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8) # torch.quantization.QuantStub()\n",
    "dequant = torch.dequantize\n",
    "\n",
    "# 量化输入和权重\n",
    "# inp_quant = quant(inp)\n",
    "# weight_quant = quant(weight)\n",
    "class WrappedConv2d(torch.nn.Module):\n",
    "    def __init__(self, conv_layer, W_bits=3, A_bits=3,quantize = False):\n",
    "        super().__init__()\n",
    "        self.align_num = 12\n",
    "        if W_bits+A_bits<3:\n",
    "            self.align_num = 24\n",
    "        self.conv = conv_layer\n",
    "        self.quantize = quantize\n",
    "        self.quant = lambda x: torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8) \n",
    "        self.dequant = lambda x: x.dequantize()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # with MeasureExecutionTime(measure_name=\"Padding inp\"):\n",
    "        # 此处还需要对inp做padding用于计算\n",
    "        padding = 2\n",
    "        W = inp.size(3)\n",
    "        W = int((W//self.align_num)*self.align_num+((W%self.align_num)>0)*self.align_num) + padding\n",
    "        # employ the new W to padding inp (with zero)\n",
    "        if W!=inp.size(3):\n",
    "            if self.quantize:\n",
    "                inp = self.dequant(inp)\n",
    "            inp = torch.nn.functional.pad(inp, (0, W-inp.size(3), 0, 0), mode='constant', value=0)\n",
    "            if self.quantize:\n",
    "                inp = self.quant(inp)\n",
    "        # print(\"input shape: \",inp.shape)\n",
    "        output = self.conv(inp)\n",
    "        # print(\"output shape: \",output.shape)\n",
    "        return output[:, :, 1:2+inp.size(2), 1:2+inp.size(2)]\n",
    "    \n",
    "# 量化卷积操作\n",
    "# 注意：这里我们需要一个量化的conv2d函数，但PyTorch不直接提供一个简单的API来量化然后执行conv2d\n",
    "# 因此，我们使用量化模型的概念来近似这个过程\n",
    "class QuantizedConv2d(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantizedConv2d, self).__init__()\n",
    "        # self.conv1 = torch.nn.Conv2d(Ci, Co, K, stride=1, padding=2, dilation=1, groups=1)\n",
    "        # self.conv2 = torch.nn.Conv2d(Ci, Co, K, stride=1, padding=2, dilation=1, groups=1)\n",
    "        self.conv1 = PadConv2d(Ci,Co,K,W_bits =W_bits,A_bits=A_bits,stride=1, dilation=1, groups=1)\n",
    "        self.conv2 = PadConv2d(Ci,Co,K,W_bits =W_bits,A_bits=A_bits,stride=1, dilation=1, groups=1)\n",
    "        # self.quant = QuantStub()\n",
    "        # self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "class QuantizedConv2d_2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        src_conv1 = torch.nn.Conv2d(Ci, Co, K, stride=1, padding=1, dilation=1, groups=1)\n",
    "        src_conv2 = torch.nn.Conv2d(Ci, Co, K, stride=1, padding=1, dilation=1, groups=1)\n",
    "        self.conv1 = WrappedConv2d(src_conv1,W_bits =W_bits,A_bits=A_bits,quantize=True)\n",
    "        self.conv2 = WrappedConv2d(src_conv2,W_bits =W_bits,A_bits=A_bits,quantize=True)\n",
    "        # self.quant = QuantStub()\n",
    "        # self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "class MyQuantizedConv2d(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.conv = torch.nn.Conv2d(Ci, Co, K, stride=1, padding=4, dilation=1, groups=1)\n",
    "        # self.quant = QuantStub()\n",
    "        self.conv1 = DirectConv2d(Ci, Co, K, 3, 3, True, 0, 1, 1, measure_time=False)\n",
    "        self.conv2 = DirectConv2d(Ci, Co, K, 3, 3, True, 0, 1, 1, measure_time=False)\n",
    "        # self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# 将权重复制到量化模型中\n",
    "model = QuantizedConv2d()\n",
    "# model.conv1.weight = torch.nn.Parameter(weight)\n",
    "# model.conv1.bias = None  # 假设没有偏置\n",
    "\n",
    "mymodel = MyQuantizedConv2d()\n",
    "\n",
    "batch_size = 16\n",
    "test_iters = 10\n",
    "shape = (batch_size, Ci, H, W)\n",
    "Ho = H+2\n",
    "Wo = W+2\n",
    "Flops = 2 * batch_size * Co * Ho * Wo * Ci * K * K\n",
    "# Function to test the inference speed\n",
    "def test_inference_speed(model,input_shape,qunantize_input=False):\n",
    "    ecalps_time = 0\n",
    "    with torch.no_grad():\n",
    "        # for images, labels in testloader:\n",
    "        for i in tqdm(range(test_iters)):\n",
    "            images = torch.rand(*input_shape)\n",
    "            if qunantize_input:\n",
    "                images = quant(images)\n",
    "            start_time = time.time()\n",
    "            outputs = model(images)\n",
    "            m = outputs.mean().item()\n",
    "            ecalps_time += time.time()-start_time\n",
    "    return ecalps_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "fp32_inference_time = test_inference_speed(model,shape,qunantize_input=False)\n",
    "print(f'FP32 inference time: {fp32_inference_time:.2f} seconds, FPS: {batch_size*test_iters/fp32_inference_time:.2f}, GFLOPS: {Flops*test_iters/fp32_inference_time/1e9:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:08<00:00,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our inference time: 58.83 seconds, FPS: 2.72, GFLOPS: 10.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "my_inference_time = test_inference_speed(mymodel,shape,qunantize_input=False)\n",
    "print(f'Our inference time: {my_inference_time:.2f} seconds, FPS: {batch_size*test_iters/my_inference_time:.2f}, GFLOPS: {Flops*test_iters/my_inference_time/1e9:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/newhome/gongcheng/.local/lib/python3.9/site-packages/torch/ao/quantization/utils.py:339: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAGER INT8 inference time: 42.92 seconds, FPS: 3.73, GFLOPS: 14.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from torch.quantization import QConfig\n",
    "import torch.nn.quantized as nnq\n",
    "\n",
    "model_2 = QuantizedConv2d()\n",
    "# 量化模型准备和转换\n",
    "engine_name = 'qnnpack' # 'fbgemm' # 'qnnpack'\n",
    "# Set the quantization engine to QNNPACK\n",
    "torch.backends.quantized.engine = engine_name\n",
    "# qconfig = torch.quantization.get_default_qconfig(engine_name)\n",
    "qconfig = QConfig(\n",
    "    weight=torch.quantization.default_observer.with_args(dtype=torch.qint8),  # Use quint8 for weights\n",
    "    activation=torch.quantization.default_observer.with_args(dtype=torch.quint8)  # Use quint8 for activations\n",
    ")\n",
    "\n",
    "q_model = copy.deepcopy(model_2)\n",
    "# Prepare the model for quantization\n",
    "q_model.qconfig = qconfig \n",
    "torch.quantization.prepare(q_model, inplace=True)\n",
    "# Quantize the model\n",
    "# run_model_on_data(model, data)\n",
    "# calibra_data = torch.rand(4,3,32,32)\n",
    "# q_model(calibra_data)\n",
    "torch.quantization.convert(q_model, inplace=True)\n",
    "\n",
    "# start = time.time()\n",
    "# output_quant = model(inp_quant)\n",
    "# escap_time = time.time() - start\n",
    "\n",
    "# 性能测量\n",
    "# print(\"Time taken: \", escap_time)\n",
    "# flops = 2*N*Ci*H*W*Co*K*K\n",
    "# print(\"FLOPS: \", flops)\n",
    "# print(\"GFLOPS: \", flops/escap_time/1e9)\n",
    "\n",
    "int8_inference_time = test_inference_speed(q_model,shape,qunantize_input=True)\n",
    "print(f'EAGER INT8 inference time: {int8_inference_time:.2f} seconds, FPS: {batch_size*test_iters/int8_inference_time:.2f}, GFLOPS: {Flops*test_iters/int8_inference_time/1e9:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 47.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FX int8 inference time: 0.19815945625305176 seconds, FPS: 807.4305563075338, GFLOPS: 2.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization import (\n",
    "  get_default_qconfig_mapping,\n",
    "  get_default_qat_qconfig_mapping,\n",
    "  QConfigMapping,\n",
    ")\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "import copy\n",
    "\n",
    "# fx module for quantization\n",
    "model_fp = copy.deepcopy(model)\n",
    "\n",
    "# post training static quantization\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "# fusion\n",
    "model_to_quantize.eval()\n",
    "model_to_quantize = quantize_fx.fuse_fx(model_to_quantize)\n",
    "qconfig_mapping = get_default_qconfig_mapping(\"onednn\")\n",
    "example_inputs = torch.rand((1,Ci,H,W))\n",
    "# prepare\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
    "# calibrate (not shown)\n",
    "# model_prepared(example_inputs)\n",
    "# quantize\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "\n",
    "fx_int8_inference_time = test_inference_speed(q_model,shape,qunantize_input=True)\n",
    "print(f'FX int8 inference time: {fx_int8_inference_time} seconds, FPS: {batch_size*test_iters/fx_int8_inference_time}, GFLOPS: {Flops*test_iters/fx_int8_inference_time/1e9:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
