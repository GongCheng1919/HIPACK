{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float time: 3.0160 s\n",
      "Qint8 time: 881.0310 ms\n",
      "DC time: 1.0148 s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys, os\n",
    "notebook_path = os.getcwd()  # Get the current working directory\n",
    "parent_directory = os.path.dirname(notebook_path)  # Get the parent directory\n",
    "sys.path.append(parent_directory)\n",
    "# from models.resnet import ResNet18\n",
    "# from models.vggnet import ExpVGG16_BN\n",
    "from models.utils import eager_quantize_model,fx_quantize_model,MeasureExecutionTime\n",
    "from models.conv import fuse_module\n",
    "from models.mobilenetv2 import mobilenet_v2,mobilenet_v2_qint8,mobilenet_v2_dc\n",
    "# 设置量化引擎为QNNPACK\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "x = torch.rand(16,3,224,224)\n",
    "model =mobilenet_v2()\n",
    "q_model = mobilenet_v2_qint8()\n",
    "dc_model = mobilenet_v2_dc(5,5)\n",
    "with MeasureExecutionTime(measure_name=\"Float\"):\n",
    "    out= model(x)\n",
    "\n",
    "with MeasureExecutionTime(measure_name=\"Qint8\"):\n",
    "    out= q_model(x)\n",
    "\n",
    "with MeasureExecutionTime(measure_name=\"DC\"):\n",
    "    out= dc_model(x)\n",
    "\n",
    "# vgg16bn = ExpVGG16_BN()\n",
    "# convert_to_direct_conv2d(fake_fuse_module_bn_relu(resnet18),3,3, module_type=nn.Conv2d)\n",
    "# vgg16bn(torch.rand(1,3,224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m(torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# model(torch.rand(1,3,224,224))\n",
    "def test_packed_weights(values,guard_bits=13):\n",
    "    values=[]\n",
    "    for i in range(3):\n",
    "        values.append((values>>(i*guard_bits))&((1<<guard_bits)-1))\n",
    "    return values\n",
    "test_packed_weights(8+1<<13+2<<26,13)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExpVGG(\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       "  (features): Sequential(\n",
       "    (0): AlignW()\n",
       "    (1): ConvReLU2d(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Identity()\n",
       "    (3): Identity()\n",
       "    (4): AlignW()\n",
       "    (5): ConvReLU2d(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Identity()\n",
       "    (7): Identity()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): AlignW()\n",
       "    (10): ConvReLU2d(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Identity()\n",
       "    (12): Identity()\n",
       "    (13): AlignW()\n",
       "    (14): ConvReLU2d(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Identity()\n",
       "    (16): Identity()\n",
       "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (18): AlignW()\n",
       "    (19): ConvReLU2d(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Identity()\n",
       "    (21): Identity()\n",
       "    (22): AlignW()\n",
       "    (23): ConvReLU2d(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (24): Identity()\n",
       "    (25): Identity()\n",
       "    (26): AlignW()\n",
       "    (27): ConvReLU2d(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (28): Identity()\n",
       "    (29): Identity()\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (31): AlignW()\n",
       "    (32): ConvReLU2d(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (33): Identity()\n",
       "    (34): Identity()\n",
       "    (35): AlignW()\n",
       "    (36): ConvReLU2d(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (37): Identity()\n",
       "    (38): Identity()\n",
       "    (39): AlignW()\n",
       "    (40): ConvReLU2d(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (41): Identity()\n",
       "    (42): Identity()\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (44): AlignW()\n",
       "    (45): ConvReLU2d(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (46): Identity()\n",
       "    (47): Identity()\n",
       "    (48): AlignW()\n",
       "    (49): ConvReLU2d(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (50): Identity()\n",
       "    (51): Identity()\n",
       "    (52): AlignW()\n",
       "    (53): ConvReLU2d(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (54): Identity()\n",
       "    (55): Identity()\n",
       "    (56): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): LinearReLU(\n",
       "      (0): Linear(in_features=43008, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Identity()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): LinearReLU(\n",
       "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Identity()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from models.vggnet import VGG16_BN_qint8\n",
    "# vgg16bn_qint8 = VGG16_BN_qint8()\n",
    "# vgg16bn_qint8(torch.rand(1,3,224,224))\n",
    "# vgg16bn_qint8\n",
    "# from models.utils import eager_quantize_model,fx_quantize_model\n",
    "from models.conv import fuse_module\n",
    "# vgg16bn.eval()\n",
    "model = fuse_module(vgg16bn)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# import sys, os\n",
    "# notebook_path = os.getcwd()  # Get the current working directory\n",
    "# parent_directory = os.path.dirname(notebook_path)  # Get the parent directory\n",
    "# sys.path.append(parent_directory)\n",
    "# from models.utils import eager_quantize_model,fx_quantize_model\n",
    "# q_model = eager_quantize_model(model,shape=(1,3,224,224))\n",
    "# import torch\n",
    "# torch.__version__\n",
    "# vgg16bn_qint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/newhome/gongcheng/.local/lib/python3.9/site-packages/torch/ao/quantization/utils.py:339: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from models.vggnet import ExpVGG16_BN_qint8\n",
    "vgg16bn_qint8 = ExpVGG16_BN_qint8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th layer:\n",
      "\teagerqint8 layer  time: 0.02710368880070746\n",
      "\tdirectconv layer  time: 0.052724953508004546\n",
      "\t Layer features.1: eagerqint8 is faster than directconv W3A3, keep it!\n",
      "1-th layer:\n",
      "\teagerqint8 layer  time: 0.15064069454092532\n",
      "\tdirectconv layer  time: 0.18505668843863532\n",
      "\t Layer features.5: eagerqint8 is faster than directconv W3A3, keep it!\n",
      "2-th layer:\n",
      "\teagerqint8 layer  time: 0.07917620684020221\n",
      "\tdirectconv layer  time: 0.08322201180271804\n",
      "\t Layer features.10: eagerqint8 is faster than directconv W3A3, keep it!\n",
      "3-th layer:\n",
      "\teagerqint8 layer  time: 0.18105861451476812\n",
      "\tdirectconv layer  time: 0.1408108095638454\n",
      "\t Layer features.14: directconv W3A3 is faster than eagerqint8 by 0.04025s, replace it!\n",
      "4-th layer:\n",
      "\teagerqint8 layer  time: 0.07793318771291524\n",
      "\tdirectconv layer  time: 0.06350989069323987\n",
      "\t Layer features.19: directconv W3A3 is faster than eagerqint8 by 0.01442s, replace it!\n",
      "5-th layer:\n",
      "\teagerqint8 layer  time: 0.14332643075613305\n",
      "\tdirectconv layer  time: 0.11414974753279239\n",
      "\t Layer features.23: directconv W3A3 is faster than eagerqint8 by 0.02918s, replace it!\n",
      "6-th layer:\n",
      "\teagerqint8 layer  time: 0.1470187708036974\n",
      "\tdirectconv layer  time: 0.13339294074103236\n",
      "\t Layer features.27: directconv W3A3 is faster than eagerqint8 by 0.01363s, replace it!\n",
      "7-th layer:\n",
      "\teagerqint8 layer  time: 0.09918492252472788\n",
      "\tdirectconv layer  time: 0.06682908948278055\n",
      "\t Layer features.32: directconv W3A3 is faster than eagerqint8 by 0.03236s, replace it!\n",
      "8-th layer:\n",
      "\teagerqint8 layer  time: 0.1759982428047806\n",
      "\tdirectconv layer  time: 0.1431536822929047\n",
      "\t Layer features.36: directconv W3A3 is faster than eagerqint8 by 0.03284s, replace it!\n",
      "9-th layer:\n",
      "\teagerqint8 layer  time: 0.1636223712703213\n",
      "\tdirectconv layer  time: 0.13698197569465265\n",
      "\t Layer features.40: directconv W3A3 is faster than eagerqint8 by 0.02664s, replace it!\n",
      "10-th layer:\n",
      "\teagerqint8 layer  time: 0.07045373122673482\n",
      "\tdirectconv layer  time: 0.06262609153054655\n",
      "\t Layer features.45: directconv W3A3 is faster than eagerqint8 by 0.007828s, replace it!\n",
      "11-th layer:\n",
      "\teagerqint8 layer  time: 0.07754516269778833\n",
      "\tdirectconv layer  time: 0.05952841875841841\n",
      "\t Layer features.49: directconv W3A3 is faster than eagerqint8 by 0.01802s, replace it!\n",
      "12-th layer:\n",
      "\teagerqint8 layer  time: 0.06740246072877198\n",
      "\tdirectconv layer  time: 0.05857683776412159\n",
      "\t Layer features.53: directconv W3A3 is faster than eagerqint8 by 0.008826s, replace it!\n",
      "0-th layer:\n",
      "\teagerqint8 layer  time: 0.0236533290008083\n",
      "\tdirectconv layer  time: 0.03907796082785353\n",
      "\t Layer features.1: eagerqint8 is faster than directconv W2A2, keep it!\n",
      "1-th layer:\n",
      "\teagerqint8 layer  time: 0.14624165574787185\n",
      "\tdirectconv layer  time: 0.1649291257490404\n",
      "\t Layer features.5: eagerqint8 is faster than directconv W2A2, keep it!\n",
      "2-th layer:\n",
      "\teagerqint8 layer  time: 0.07783442473737523\n",
      "\tdirectconv layer  time: 0.09249118203297257\n",
      "\t Layer features.10: eagerqint8 is faster than directconv W2A2, keep it!\n",
      "3-th layer:\n",
      "\teagerqint8 layer  time: 0.16708193527301773\n",
      "\tdirectconv layer  time: 0.1424155576969497\n",
      "\t Layer features.14: directconv W2A2 is faster than eagerqint8 by 0.02467s, replace it!\n",
      "4-th layer:\n",
      "\teagerqint8 layer  time: 0.07018103887094185\n",
      "\tdirectconv layer  time: 0.07003551628440619\n",
      "\t Layer features.19: directconv W2A2 is faster than eagerqint8 by 0.0001455s, replace it!\n",
      "5-th layer:\n",
      "\teagerqint8 layer  time: 0.16088840446900576\n",
      "\tdirectconv layer  time: 0.14707439503399655\n",
      "\t Layer features.23: directconv W2A2 is faster than eagerqint8 by 0.01381s, replace it!\n",
      "6-th layer:\n",
      "\teagerqint8 layer  time: 0.1692420957260765\n",
      "\tdirectconv layer  time: 0.12653032696107402\n",
      "\t Layer features.27: directconv W2A2 is faster than eagerqint8 by 0.04271s, replace it!\n",
      "7-th layer:\n",
      "\teagerqint8 layer  time: 0.07971548498608172\n",
      "\tdirectconv layer  time: 0.056090957252308726\n",
      "\t Layer features.32: directconv W2A2 is faster than eagerqint8 by 0.02362s, replace it!\n",
      "8-th layer:\n",
      "\teagerqint8 layer  time: 0.1715256999596022\n",
      "\tdirectconv layer  time: 0.11876024270895869\n",
      "\t Layer features.36: directconv W2A2 is faster than eagerqint8 by 0.05277s, replace it!\n",
      "9-th layer:\n",
      "\teagerqint8 layer  time: 0.16998792823869735\n",
      "\tdirectconv layer  time: 0.13960822543594986\n",
      "\t Layer features.40: directconv W2A2 is faster than eagerqint8 by 0.03038s, replace it!\n",
      "10-th layer:\n",
      "\teagerqint8 layer  time: 0.06298606126802042\n",
      "\tdirectconv layer  time: 0.04325786221306771\n",
      "\t Layer features.45: directconv W2A2 is faster than eagerqint8 by 0.01973s, replace it!\n",
      "11-th layer:\n",
      "\teagerqint8 layer  time: 0.059203125070780516\n",
      "\tdirectconv layer  time: 0.04550672124605626\n",
      "\t Layer features.49: directconv W2A2 is faster than eagerqint8 by 0.0137s, replace it!\n",
      "12-th layer:\n",
      "\teagerqint8 layer  time: 0.07140488148434088\n",
      "\tdirectconv layer  time: 0.051427374011836946\n",
      "\t Layer features.53: directconv W2A2 is faster than eagerqint8 by 0.01998s, replace it!\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.nn.intrinsic.quantized.modules.conv_relu import ConvReLU2d\n",
    "# isinstance(, type(vgg16bn_qint8.features))\n",
    "type(vgg16bn_qint8.features[0])\n",
    "# vgg16bn_qint8\n",
    "import time\n",
    "def test_time(layer,input,warmup=4,repeat = 10):\n",
    "    for _ in range(warmup):\n",
    "        output = layer(input)\n",
    "    \n",
    "    elapsed_time = 0\n",
    "    for _ in range(repeat):\n",
    "        start = time.perf_counter()\n",
    "        output = layer(input)\n",
    "        elapsed_time += time.perf_counter() - start\n",
    "    return output, elapsed_time/repeat\n",
    "\n",
    "import torch\n",
    "def get_module_input_output_shape(model, \n",
    "                                  layer_types=(torch.nn.Conv2d,), \n",
    "                                  first_input_shape=(1,3,224,224),\n",
    "                                  qunantize_input=False,\n",
    "                                  ):\n",
    "    \"\"\"\n",
    "    Get the input shape for a specified layer in a PyTorch model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The PyTorch model.\n",
    "    - layer_name: The name of the layer for which to find the input shape.\n",
    "    - input_shape: The shape of the input to feed to the model for the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    - The input shape to the specified layer.\n",
    "    \"\"\"\n",
    "    # Variable to store the input shape\n",
    "    captured_input_shapes = []\n",
    "    captured_output_shapes = []\n",
    "    captured_layers = []\n",
    "    captured_layer_names = []\n",
    "    handles = []\n",
    "\n",
    "    # Hook function to capture the input shape\n",
    "    def hook_fn(module, input, output):\n",
    "        # nonlocal captured_shape\n",
    "        captured_input_shapes.append(input[0].shape)\n",
    "        captured_output_shapes.append(output.shape)\n",
    "        # captured_layers.append(module)\n",
    "\n",
    "    # Find and attach the hook to the specified layer\n",
    "    for name, layer in model.named_modules():\n",
    "        # print(layer_types)\n",
    "        if isinstance(layer, layer_types):\n",
    "            handles.append(layer.register_forward_hook(hook_fn))\n",
    "            captured_layer_names.append(name)\n",
    "            captured_layers.append(layer)\n",
    "            # break\n",
    "        # else:\n",
    "        #     raise ValueError(f\"Layer {layer_types} not found in the model.\")\n",
    "\n",
    "    # Create a dummy input and perform a forward pass to trigger the hook\n",
    "    dummy_input = torch.rand(first_input_shape)\n",
    "    if qunantize_input:\n",
    "        quant = lambda x: torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8) # torch.quantization.QuantStub()\n",
    "        dummy_input = quant(dummy_input)\n",
    "    model(dummy_input)\n",
    "\n",
    "    # Remove the hook\n",
    "    # handle.remove()\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "    return captured_layer_names, captured_layers, captured_input_shapes, captured_output_shapes\n",
    "\n",
    "from models.conv import DirectConv2d,PadConv2d\n",
    "def create_direct_conv2d_from(module, W_bits=3, A_bits=3, type=\"directconv\",\n",
    "                              prepare_func=lambda x: x.int(),\n",
    "                             post_func=lambda x: x,\n",
    "                             copy_weight = False,\n",
    "                             quantize = False):\n",
    "    __types__ = [\"directconv\",\"padconv\"]\n",
    "    if type not in __types__:\n",
    "        raise ValueError(f\"Only support {__types__} for type, but got {type}\")\n",
    "    dc_layer = None\n",
    "    if module.kernel_size[0]==3 \\\n",
    "                and module.stride[0]==1 and module.dilation[0]==1:\n",
    "        if type == \"directconv\":\n",
    "            dc_layer = DirectConv2d(module.in_channels, \n",
    "                        module.out_channels, \n",
    "                        module.kernel_size[0], \n",
    "                        W_bits, A_bits, True,\n",
    "                        prepare_func, post_func)\n",
    "        elif type == \"padconv\":\n",
    "            dc_layer = PadConv2d(module.in_channels, \n",
    "                                module.out_channels, \n",
    "                                module.kernel_size[0], \n",
    "                                W_bits = W_bits, A_bits = A_bits, \n",
    "                                stride= module.stride[0], \n",
    "                                dilation=module.dilation[0])\n",
    "    return dc_layer\n",
    "\n",
    "def replace_module(model, path, new_module):\n",
    "    \"\"\"\n",
    "    递归地替换模型中的模块。\n",
    "    :param model: 要修改的模型。\n",
    "    :param path: 要替换的模块的路径，以点分隔。\n",
    "    :param new_module: 新的模块对象。\n",
    "    \"\"\"\n",
    "    path_list = path.split('.')\n",
    "    for name in path_list[:-1]:\n",
    "        model = getattr(model, name)\n",
    "    setattr(model, path_list[-1], new_module)\n",
    "\n",
    "import copy\n",
    "\n",
    "def TestandReplace_VGGlayer(model, W_A_bit,layer_names, layers, input_shapes):\n",
    "    repeat = 4\n",
    "    warmup = 2\n",
    "    W_bits, A_bits = W_A_bit\n",
    "    replace_layers = []\n",
    "    start_layer_list = []\n",
    "    end_layer_list = []\n",
    "    end_layer_names = []\n",
    "    pre_replace_layer = None\n",
    "    for i,layer in enumerate(layers):\n",
    "        # print(layer)\n",
    "        # print(input_shapes[i])\n",
    "        # print(output_shapes[i])\n",
    "        # if isinstance(layer, ConvReLU2d):\n",
    "        # 测试以下两个层的运行耗时\n",
    "        inp = torch.rand(input_shapes[i])\n",
    "        quant_inp = torch.quantize_per_tensor(inp, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "        direct_conv2d_layer = create_direct_conv2d_from(layer, W_bits, A_bits,type=\"directconv\")\n",
    "        # pad_conv2d_layer = create_direct_conv2d_from(layer, W_bits, A_bits,type=\"wrappedconv\",quantize = True)\n",
    "        out1,time1 = test_time(layer, quant_inp, warmup=warmup,repeat = repeat)\n",
    "        out2,time2 = test_time(direct_conv2d_layer, inp, warmup=warmup,repeat = repeat)\n",
    "        # out3,time3 = test_time(pad_conv2d_layer, inp)\n",
    "        print(f\"{i}-th layer:\")\n",
    "        print(f\"\\teagerqint8 layer  time: {time1}\")\n",
    "        print(f\"\\tdirectconv layer  time: {time2}\")\n",
    "        \n",
    "        if (time1>time2): # 将原始layer替换为directconv\n",
    "            print(f\"\\t Layer {layer_names[i]}: directconv W{W_bits}A{A_bits} is faster than eagerqint8 by {time1-time2:.4}s, replace it!\")\n",
    "            # setattr(model, layer_names[i], direct_conv2d_layer)\n",
    "            replace_module(model, layer_names[i], direct_conv2d_layer)\n",
    "            replace_layers.append(direct_conv2d_layer)\n",
    "            if pre_replace_layer is None:\n",
    "                pre_replace_layer = direct_conv2d_layer\n",
    "                start_layer_list.append(pre_replace_layer)\n",
    "                layer_names.append(layer_names[i])\n",
    "        else:\n",
    "            print(f\"\\t Layer {layer_names[i]}: eagerqint8 is faster than directconv W{W_bits}A{A_bits}, keep it!\")\n",
    "            if pre_replace_layer is not None:\n",
    "                end_layer_list.append(pre_replace_layer)\n",
    "                pre_replace_layer = None\n",
    "                # print(end_layer_name = layer_names[i])\n",
    "        # print(f\"\\twrappedconv layer  time: {time3}\")\n",
    "    # 记得把最后一层的输出改为qint8\n",
    "    end_layer_list.append(replace_layers[-1])\n",
    "    for layer in start_layer_list:\n",
    "        layer.prepare_func = lambda x: x.dequantize().int()\n",
    "    # model.features[10].prepare_func = lambda x: x.dequantize().int()\n",
    "    for layer in end_layer_list:\n",
    "        layer.post_func = lambda x: torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "    # model.features[53].post_func = lambda x: torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "    return model\n",
    "\n",
    "def VGG_BN_directconv(W_A_bit_list=[3,3], name = \"VGG16\",shape=(1,3,224,224),engine_name = 'qnnpack'):\n",
    "    # model = VGG_BN_qint8(name,shape,engine_name)\n",
    "    model = copy.deepcopy(vgg16bn_qint8)\n",
    "    # 测试每一个量化卷积层和对应的directconv的耗时，并替换计算更快的卷积层\n",
    "    \n",
    "    layer_names, layers, input_shapes, output_shapes = get_module_input_output_shape(model, \n",
    "                                                                        qunantize_input=True,\n",
    "                                                                        layer_types=(ConvReLU2d,), \n",
    "                                                                        first_input_shape=shape)\n",
    "    # repeat = 4\n",
    "    # warmup = 2\n",
    "    # for i,layer in enumerate(layers):\n",
    "    #     # print(layer)\n",
    "    #     # print(input_shapes[i])\n",
    "    #     # print(output_shapes[i])\n",
    "    #     # if isinstance(layer, ConvReLU2d):\n",
    "    #     # 测试以下两个层的运行耗时\n",
    "    #     inp = torch.rand(input_shapes[i])\n",
    "    #     quant_inp = torch.quantize_per_tensor(inp, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "    #     direct_conv2d_layer = create_direct_conv2d_from(layer, W_bits, A_bits,type=\"directconv\")\n",
    "    #     # pad_conv2d_layer = create_direct_conv2d_from(layer, W_bits, A_bits,type=\"wrappedconv\",quantize = True)\n",
    "    #     out1,time1 = test_time(layer, quant_inp, warmup=warmup,repeat = repeat)\n",
    "    #     out2,time2 = test_time(direct_conv2d_layer, inp, warmup=warmup,repeat = repeat)\n",
    "    #     # out3,time3 = test_time(pad_conv2d_layer, inp)\n",
    "    #     print(f\"{i}-th layer:\")\n",
    "    #     print(f\"\\teagerqint8 layer  time: {time1}\")\n",
    "    #     print(f\"\\tdirectconv layer  time: {time2}\")\n",
    "    #     if (time1>time2): # 将原始layer替换为directconv\n",
    "    #         print(f\"\\t Layer {layer_names[i]}: directconv is faster than eagerqint8 by {time1-time2:.4}s, replace it!\")\n",
    "    #         # setattr(model, layer_names[i], direct_conv2d_layer)\n",
    "    #         replace_module(model, layer_names[i], direct_conv2d_layer)\n",
    "    #     else:\n",
    "    #         print(f\"\\t Layer {layer_names[i]}: eagerqint8 is faster than directconv, keep it!\")\n",
    "    #     # print(f\"\\twrappedconv layer  time: {time3}\")\n",
    "    # model.features[10].prepare_func = lambda x: x.dequantize().int()\n",
    "    # model.features[53].post_func = lambda x: torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "    models = [copy.deepcopy(model) for _ in W_A_bit_list]\n",
    "    for m,W_A_bits in zip(models,W_A_bit_list):\n",
    "        m = TestandReplace_VGGlayer(m, W_A_bits, layer_names, layers, input_shapes)\n",
    "    return models\n",
    "# layer_names, layers, input_shapes, output_shapes = get_module_input_output_shape(vgg16bn_qint8, \n",
    "#                                                             qunantize_input=True,\n",
    "#                                                             layer_types=(ConvReLU2d,), \n",
    "#                                                             first_input_shape=(1,3,224,224))\n",
    "\n",
    "W_A_bit_list = [[3,3],[2,2]]\n",
    "vgg16bn_directconvs = VGG_BN_directconv(W_A_bit_list=W_A_bit_list,shape=(1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0134, -0.0030, -0.0022,  ..., -0.0090,  0.0194, -0.0134],\n",
       "        [-0.0134, -0.0030, -0.0022,  ..., -0.0090,  0.0194, -0.0134],\n",
       "        [-0.0134, -0.0030, -0.0022,  ..., -0.0090,  0.0194, -0.0134],\n",
       "        [-0.0134, -0.0030, -0.0022,  ..., -0.0090,  0.0194, -0.0134]],\n",
       "       size=(4, 1000), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.0003731698670890182,\n",
       "       zero_point=64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0](quant(torch.rand(4,3,224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "\tmodel[0]: latency: 1.354s, fps: 0.7387 \n",
      "\tmodel[1]: latency: 1.432s, fps: 0.6981 \n",
      "Batch size: 2\n",
      "\tmodel[0]: latency: 2.442s, fps: 0.819 \n",
      "\tmodel[1]: latency: 2.22s, fps: 0.9011 \n",
      "Batch size: 4\n",
      "\tmodel[0]: latency: 3.631s, fps: 1.102 \n",
      "\tmodel[1]: latency: 3.591s, fps: 1.114 \n",
      "Batch size: 8\n",
      "\tmodel[0]: latency: 6.342s, fps: 1.261 \n",
      "\tmodel[1]: latency: 6.312s, fps: 1.267 \n"
     ]
    }
   ],
   "source": [
    "# quant = lambda x: torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8) \n",
    "# vgg16bn_directconv(quant(torch.rand(1,3,224,224)))\n",
    "batch_sizes = [1,2,4,8]\n",
    "perf_comp = {}\n",
    "model_names = [f\"vgg16bn_directconv_W{v[0]}A{v[0]}\" for v in W_A_bit_list]\n",
    "models = vgg16bn_directconvs\n",
    "# models[0].features[53].post_func = lambda x: torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "# models[1].features[53].post_func = lambda x: torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "# models[2].features[53].post_func = lambda x: torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8)\n",
    "import time\n",
    "for b in batch_sizes:\n",
    "    print(f\"Batch size: {b}\")\n",
    "    perf_comp[b] = []\n",
    "    for i, m in enumerate(models):\n",
    "        # out0,time0 = test_time(vgg16bn, torch.rand(b,3,224,224),1,2)\n",
    "        # out1,time1 = test_time(vgg16bn_qint8, quant(torch.rand(b,3,224,224)),1,2)\n",
    "        out,ecaplated_time = test_time(m, quant(torch.rand(b,3,224,224)),2,4)\n",
    "        # fps0 = b/time0\n",
    "        # fps1 = b/time1\n",
    "        fps = b/ecaplated_time\n",
    "        res=[ecaplated_time,fps]\n",
    "        # print(f\"\\tvgg16bn_float: latency: {time0:.4}s, fps: {fps0:.4} \")\n",
    "        # print(f\"\\tvgg16bn_qint8: latency: {time1:.4}s, fps: {fps1:.4} \")\n",
    "        print(f\"\\t{model_names[i]}: latency: {ecaplated_time:.4}s, fps: {fps:.4} \")\n",
    "        perf_comp[b].append(fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([0.20987828, 0.71625762, 0.74900621]),\n",
       " 2: array([0.19773487, 0.74032495, 0.92111343]),\n",
       " 4: array([0.13423196, 0.94447353, 1.01818722]),\n",
       " 8: array([0.13565108, 1.0991052 , 1.32400622])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out2,time2 = test_time(vgg16bn_directconv, quant(torch.rand(1,3,224,224)),2,4)\n",
    "# time2\n",
    "\n",
    "import numpy as np\n",
    "perf_comp_b = {b:int(b)*np.array(perf_comp[b][3:]) for b in batch_sizes}\n",
    "perf_comp_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3640695284702815"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qnnpack: 1.7150103444000706\n",
    "directconv: 1.3640695284702815\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct_conv2d_layer = create_direct_conv2d_from(layers[0], 3, 3,type=\"directconv\")\n",
    "# direct_conv2d_layer\n",
    "# x = quant(torch.rand(1,3,224,224))\n",
    "# x.dtype == torch.int32\n",
    "# vgg16bn_directconv.features[10].prepare_func = lambda x: x.dequantize().int()\n",
    "# x.dequantize().int()\n",
    "vgg16bn_directconv.features[53].post_func = lambda x: torch.quantize_per_tensor(x, scale=1.0, zero_point=0, dtype=torch.quint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ConvReLU2d\n",
    "# from torch.ao.nn.intrinsic.quantized.modules.conv_relu import ConvReLU2d\n",
    "# isinstance(vgg16bn_qint8.features[0],ConvReLU2d)\n",
    "# layers, input_shapes, output_shapes = get_module_input_output_shape(vgg16bn_qint8, \n",
    "#                                                                         qunantize_input=True,\n",
    "#                                                                         layer_types=(ConvReLU2d,), \n",
    "#                                                                         first_input_shape=(1,3,224,224))\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:12<00:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float VGG16 Inference time: 12.500531435012817 seconds, FPS: 0.31998639584204996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.500531435012817"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.utils import test_inference_speed\n",
    "test_inference_speed(vgg16bn, (1,3,224,224),False,4,title=\"Float VGG16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qint8 VGG16 Inference time: 4.933128833770752 seconds, FPS: 0.8108444224316977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.933128833770752"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.utils import test_inference_speed\n",
    "test_inference_speed(vgg16bn_qint8, (1,3,224,224),True,4,title=\"Qint8 VGG16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess time: Execution time: 965.5147 us\n",
      "1.5703e-05s\n",
      "direct_conv2d computation time: 0.000151721s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import direct_conv2d  # 确保已经导入direct_conv2d模块\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.quantized import functional as QF\n",
    "\n",
    "# 设置量化引擎为QNNPACK\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "\n",
    "import sys, os\n",
    "notebook_path = os.getcwd()  # Get the current working directory\n",
    "parent_directory = os.path.dirname(notebook_path)  # Get the parent directory\n",
    "sys.path.append(parent_directory)\n",
    "# from models.resnet import ResNet18\n",
    "# from models.vggnet import ExpVGG16_BN\n",
    "from models.utils import MeasureExecutionTime\n",
    "\n",
    "import time\n",
    "import logging\n",
    "\n",
    "N, C, H, W = 16, 256, 56, 56\n",
    "\n",
    "dinp=torch.randint(1,3,(N,C,H,W)).int()\n",
    "dx=torch.randint(1,3,(1,C,3,3)).int()\n",
    "with MeasureExecutionTime(measure_name = \"DC\"):\n",
    "    OUT = direct_conv2d.direct_conv2d(dinp, dx,3,3,1,1,0,1)\n",
    "\n",
    "inp=torch.randint(1,3,(N,C,H,W)).float()\n",
    "x=torch.randint(1,3,(C,1,3,3)).float()\n",
    "with MeasureExecutionTime(measure_name = \"Float\"):\n",
    "    out = F.conv2d(inp, x, padding=2,groups=C)\n",
    "\n",
    "\n",
    "# 定义量化参数\n",
    "scale = 0.1  # 比例因子\n",
    "zero_point = 0  # 零点\n",
    "\n",
    "qinp = torch.quantize_per_tensor(inp, scale, zero_point, torch.quint8)\n",
    "qx = torch.quantize_per_tensor(x, scale, zero_point, torch.qint8)\n",
    "with MeasureExecutionTime(measure_name = \"Qint8\"):\n",
    "    out = QF.conv2d(qinp, qx, None, padding=2,groups=C)\n",
    "\n",
    "\n",
    "# 如果包中有描述信息的属性（这不是标准，可能不存在）\n",
    "# print(direct_conv2d.__description__)  # 这行可能不会工作，因为__description__不是一个标准属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_ = plt.hist(OUT.data.numpy().flatten(),bins=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
